{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PyTorch Hierarchical Loss","text":"<p>This package provides functions to compute Binary Cross Entropy loss for hierarchical categories.</p> <p>The general strategy is to use the \"flat\" predictions from a general detection or classification model.   These predictions are then interpreted as conditional logit confidences.   This allows us to use the existing model architecture as-is, and interpret the output as hierarchical only changing how we interpret the predictions.</p> <p>More specifically, suppose that we have $n$ categories, and a hierarchical structure over these categories.  Suppose that our model predicts a vector $V$ for some object or image.  We interpret the logit value in index i to be:</p> <p>$$ V[i] := logit(P(\\text{category}[i] | \\text{parent}(\\text{category}[i]))) $$</p> <p>We can compute the raw conditional probability using a sigmoid function.</p> <p>$$ sigmoid(V[i]) := P(\\text{category}[i] | \\text{parent}(\\text{category}[i])) $$</p> <p>We can use these to compute marginal confidences at arbitrary locations in the hierarchy:</p> <p>$$ P(\\text{category}[i]) = P(\\text{category}[i] | \\text{parent}(\\text{category}[i])) * P(\\text{parent}(\\text{category}[i]) | \\text{parent}(\\text{parent}(\\text{category}[i]))) \\ldots $$</p> <p>Given these marginal confidences, we can compute the ordinary BCE loss.</p> <p>Likely, the point of entry users will be most interested in is the <code>Hierarchy</code> class, which handles caching various views of the hierarchy, and the <code>hierarchical_loss</code> function.  You will need to incorporate the loss into your existing training regimen.  After training, one may find useful the <code>optimal_hierarchical_path</code> and various <code>...truncate...</code> functions in <code>path_utils</code> for predictive purposes.  Note that the standard predictive \"choose the category with the highest confidence\" strategy does not work here, since the marginal probability of a child will always necessarily be less than the marginal probability of the parent.  </p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install git+https://github.com/csbrown-noaa/hierarchical_loss.git\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We would love to have your contributions that improve current functionality, fix bugs, or add new features.  See the contributing guidelines for more info.</p>"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This repository is a scientific product and is not official communication of the National Oceanic and Atmospheric Administration, or the United States Department of Commerce. All NOAA GitHub project code is provided on an \u2018as is\u2019 basis and the user assumes responsibility for its use. Any claims against the Department of Commerce or Department of Commerce bureaus stemming from the use of this GitHub project will be governed by all applicable Federal law. Any reference to specific commercial products, processes, or services by service mark, trademark, manufacturer, or otherwise, does not constitute or imply their endorsement, recommendation or favoring by the Department of Commerce. The Department of Commerce seal and logo, or the seal and logo of a DOC bureau, shall not be used in any manner to imply endorsement of any commercial product or activity by DOC or the United States Government.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Code of Conduct","text":"<p>This code of conduct was developed and adapted from the Atom code of conduct in October 2021. </p>"},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. Further details of specific enforcement policies may be posted separately.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://contributor-covenant.org/version/1/4</p>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Pull requests, bug reports, and all other forms of contribution are welcomed and highly encouraged! </p>"},{"location":"CONTRIBUTING/#contents","title":"Contents","text":"<ul> <li>Code of Conduct</li> <li>Bug Reports</li> <li>Feature Requests</li> <li>Submitting Pull Requests</li> <li>Code Review</li> <li>Coding Style</li> <li>Documentation</li> <li>Certificate of Origin</li> </ul> <p>This guide serves to set clear expectations for everyone involved with the project so that we can improve it together while also creating a welcoming space for everyone to participate. Following these guidelines will help ensure a positive experience for contributors and maintainers.</p>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"Code of Conduct","text":"<p>Please review our Code of Conduct. It is in effect at all times. We expect it to be honored by everyone who contributes to this project. Acting like an asshole will not be tolerated.</p>"},{"location":"CONTRIBUTING/#bug-reports","title":"Bug Reports","text":"<p>Please include a minimal reproducible example with your bug report.</p>"},{"location":"CONTRIBUTING/#feature-requests","title":"Feature Requests","text":"<p>Feature requests are welcome if they fit within the scope of the project.</p> <p>Feature requests that you are willing to complete are especially welcome.  </p>"},{"location":"CONTRIBUTING/#submitting-pull-requests","title":"Submitting Pull Requests","text":"<p>Please submit an issue first and get community buy-in for proposed changes before doing any work.</p> <p>Please submit PRs in the smallest possible non-breaking chunks.</p>"},{"location":"CONTRIBUTING/#code-review","title":"Code Review","text":"<p>Any code pulled into this repo should be reviewed by a maintainer.</p> <p>Remember:</p> <ul> <li> <p>Review the code, not the author. Look for and suggest improvements without disparaging or insulting the author. Provide actionable feedback and explain your reasoning.</p> </li> <li> <p>You are not your code. When your code is critiqued, questioned, or constructively criticized, remember that you are not your code. Do not take code review personally.</p> </li> </ul>"},{"location":"CONTRIBUTING/#coding-style","title":"Coding Style","text":"<p>Follow the existing style.  We use the VSCode autopep8 linter.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>Please include typing and docstrings for any classes.  We use numpy style docstrings.  Our auto-documentation is configured to parse this style, so please follow this convention.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>Please write tests for your code.  Tests should be discoverable or runnable on a file-by-file basis.  Make sure all tests pass before submitting a pull request. </p> <pre><code>python -m unittest discover tests\n</code></pre>"},{"location":"CONTRIBUTING/#certificate-of-origin","title":"Certificate of Origin","text":"<p>WHEN YOU SUBMIT CODE TO THIS REPOSITORY, YOU AGREE TO LICENSE YOUR CODE UNDER THE LICENSE</p> <p>Developer's Certificate of Origin 1.1</p> <p>By making a contribution to this project, I certify that:</p> <ol> <li>The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or</li> <li>The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or</li> <li>The contribution was provided directly to me by some other person who certified (1), (2) or (3) and I have not modified it.</li> <li>I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.</li> </ol>"},{"location":"api/coco_utils/","title":"COCO utilities","text":""},{"location":"api/coco_utils/#hierarchical_loss.coco_utils.coco_category_dist","title":"<code>coco_category_dist(coco)</code>","text":"<p>Generates a bar plot of the COCO dataset category distribution.</p> <p>This function counts all instances of each category ID in the <code>coco.anns</code> attribute, maps those IDs to their names via <code>coco.cats</code>, and generates a <code>matplotlib</code> bar plot.</p> <p>The plot is configured with count labels on top of each bar and rotated x-axis labels for readability.</p> <p>Parameters:</p> Name Type Description Default <code>coco</code> <code>COCO</code> <p>A COCO API object.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>The generated <code>matplotlib</code> Figure object containing the plot. (e.g., <code>fig = coco_category_dist(coco); fig.savefig('dist.png')</code>)</p> Source code in <code>hierarchical_loss/coco_utils.py</code> <pre><code>def coco_category_dist(coco) -&gt; Figure: # Updated return type\n    \"\"\"Generates a bar plot of the COCO dataset category distribution.\n\n    This function counts all instances of each category ID in the\n    `coco.anns` attribute, maps those IDs to their names via\n    `coco.cats`, and generates a `matplotlib` bar plot.\n\n    The plot is configured with count labels on top of each bar and\n    rotated x-axis labels for readability.\n\n    Parameters\n    ----------\n    coco : pycocotools.coco.COCO\n        A COCO API object.\n\n    Returns\n    -------\n    matplotlib.figure.Figure\n        The generated `matplotlib` Figure object containing the plot.\n        (e.g., `fig = coco_category_dist(coco); fig.savefig('dist.png')`)\n    \"\"\"\n    cats, cnt = np.unique(list(map(lambda x: x['category_id'], coco.anns.values())), return_counts=True)\n    cat_names = list(map(lambda cat_id: coco.cats[cat_id]['name'], cats))\n\n    # 1. Create a new Figure and Axes\n    fig, ax = plt.subplots()\n\n    # 2. Plot on the Axes object\n    ax.bar(cat_names, cnt)\n    for i, value in enumerate(cnt):\n        ax.text(i, value + 0.5, str(value), ha='center', va='bottom')\n\n    # 3. Configure the Axes object\n    ax.set_xticks(range(len(cat_names)))\n    ax.set_xticklabels(cat_names, rotation=45, ha='right')\n\n    # 4. Apply tight layout to the Figure\n    fig.tight_layout()\n\n    # 5. Return the Figure object\n    return fig\n</code></pre>"},{"location":"api/hierarchical_loss/","title":"Loss for hierarchical categories","text":""},{"location":"api/hierarchical_loss/#hierarchical_loss.hierarchical_loss.hierarchical_loss","title":"<code>hierarchical_loss(pred, targets, hierarchy_index)</code>","text":"<p>Computes a hierarchical cross-entropy loss.</p> <p>This function interprets the raw <code>pred</code> logits as representing the log-odds of the conditional probability <code>P(category | parent)</code>. It then calculates the marginal probability <code>P(category)</code> by multiplying all conditional probabilities along the ancestor path.</p> <p>This multiplication is done in log-space for numerical stability.</p> <p>The loss is a variant of binary cross-entropy, calculated as: <code>-[t * log(s) + (1-t) * log(1-s)]</code> where: - <code>t</code> is the expanded target (1 for the node and all ancestors, 0 otherwise). - <code>s</code> is the marginal probability, <code>P(category)</code>. - <code>log(s)</code> is <code>sum(log(P(ancestor | parent_of_ancestor)))</code>, which is   computed by <code>accumulate_hierarchy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>The raw logit predictions from the model, with shape <code>(B, D, N)</code>, where <code>N</code> is the number of classes.</p> required <code>targets</code> <code>Tensor</code> <p>The one-hot target tensor, with shape <code>(B, D, N)</code>.</p> required <code>hierarchy_index</code> <code>Tensor</code> <p>An int tensor of shape <code>(N, M)</code> mapping each node <code>i</code> to its ancestral path <code>[i, parent, grandparent, ...]</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of the same shape as <code>pred</code> containing the loss value for each prediction.</p> Source code in <code>hierarchical_loss/hierarchical_loss.py</code> <pre><code>def hierarchical_loss(\n    pred: torch.Tensor, targets: torch.Tensor, hierarchy_index: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Computes a hierarchical cross-entropy loss.\n\n    This function interprets the raw `pred` logits as representing the\n    log-odds of the *conditional* probability `P(category | parent)`.\n    It then calculates the *marginal* probability `P(category)` by\n    multiplying all conditional probabilities along the ancestor path.\n\n    This multiplication is done in log-space for numerical stability.\n\n    The loss is a variant of binary cross-entropy, calculated as:\n    `-[t * log(s) + (1-t) * log(1-s)]`\n    where:\n    - `t` is the expanded target (1 for the node and all ancestors, 0 otherwise).\n    - `s` is the *marginal* probability, `P(category)`.\n    - `log(s)` is `sum(log(P(ancestor | parent_of_ancestor)))`, which is\n      computed by `accumulate_hierarchy`.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        The raw logit predictions from the model, with shape `(B, D, N)`,\n        where `N` is the number of classes.\n    targets : torch.Tensor\n        The one-hot target tensor, with shape `(B, D, N)`.\n    hierarchy_index : torch.Tensor\n        An int tensor of shape `(N, M)` mapping each node `i` to its\n        ancestral path `[i, parent, grandparent, ...]`.\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor of the same shape as `pred` containing the loss\n        value for each prediction.\n    \"\"\"\n    # logsigmoids = log(P(category | parent))\n    logsigmoids = torch.nn.functional.logsigmoid(pred)\n    # This computes log(P(marginal)) = sum(log(P(c | p))) (Bayes' rule in log space)\n    hierarchical_summed_logsigmoids = accumulate_hierarchy(logsigmoids, hierarchy_index, torch.sum, 0.)\n    # Expand target to be 1 for the node and all its ancestors\n    hierarchical_expanded_targets = expand_target_hierarchy(targets, hierarchy_index)\n    # log(1 - s) = log(1 - P(marginal))\n    #            = log(1 - exp(log(P(marginal))))\n    hierarchical_summed_log1sigmoids = log1mexp(hierarchical_summed_logsigmoids)\n    # Standard BCE loss: -[t*log(s) + (1-t)*log(1-s)]\n    return -(\n      (hierarchical_expanded_targets * hierarchical_summed_logsigmoids) \n      + (1 - hierarchical_expanded_targets) * hierarchical_summed_log1sigmoids\n    )\n</code></pre>"},{"location":"api/hierarchy/","title":"Hierarchy class","text":""},{"location":"api/hierarchy/#hierarchical_loss.hierarchy.Hierarchy","title":"<code>Hierarchy</code>","text":"<p>Centralizes all hierarchy logic, mapping, and tensor creation.</p> Source code in <code>hierarchical_loss/hierarchy.py</code> <pre><code>class Hierarchy:\n    \"\"\"\n    Centralizes all hierarchy logic, mapping, and tensor creation.\n    \"\"\"\n    def __init__(self, \n                 raw_tree: dict[Hashable, Hashable], \n                 node_to_idx_map: dict[Hashable, int] | None = None,\n                 device: torch.device | str | None = None):\n\n        all_nodes = set(raw_tree.keys()) | set(raw_tree.values())\n\n        # 1. Build the translation maps\n        if node_to_idx_map:\n            # Use the provided map\n            self.node_to_idx = node_to_idx_map\n            # Verify all nodes are accounted for\n            for node in all_nodes:\n                if node not in self.node_to_idx:\n                    raise ValueError(f\"Node '{node}' from raw_tree is missing from the provided node_to_idx_map.\")\n            # Verify node indices are sequential and dense\n            idx_vals = node_to_idx_map.values()\n            min_idx, max_idx, n_idx = min(idx_vals), max(idx_vals), len(idx_vals)\n            if min_idx !=0 or (max_idx != n_idx-1):\n                raise ValueError(f\"node_to_idx_map must have contiguous sequential indices\")\n        else:\n            # Auto-generate a dense map\n            self.node_to_idx = {node: i for i, node in enumerate(all_nodes)}\n\n        self.idx_to_node = {i: n for n, i in self.node_to_idx.items()}\n        self.num_classes = len(self.node_to_idx)\n\n        # 2. Create the core index-based tree\n        self.index_tree = dict_keyvalue_replace(raw_tree, self.node_to_idx)\n\n        # 3. Pre-compute all tensor and dict representations\n        # These are now cached for the lifetime of the object.\n        self.parent_tensor = build_parent_tensor(self.index_tree, device=device)\n        self.index_tensor = build_hierarchy_index_tensor(self.index_tree, device=device)\n        self.hierarchy_mask = self.index_tensor == -1\n        self.sibling_mask = build_hierarchy_sibling_mask(self.parent_tensor, device=device)\n        self.roots = torch.tensor(get_roots(self.index_tree), device=device)\n        self.parent_child_tensor_tree = construct_parent_childtensor_tree(self.index_tree, device=device)\n\n    def to(self, device: torch.device | str):\n        \"\"\"Moves all computed tensors to the specified device.\"\"\"\n        self.parent_tensor = self.parent_tensor.to(device)\n        self.index_tensor = self.index_tensor.to(device)\n        self.hierarchy_mask = self.hierarchy_mask.to(device)\n        self.sibling_mask = self.sibling_mask.to(device)\n        self.roots = self.roots.to(device) \n        self.parent_child_tensor_tree = {k: v.to(device) for k, v in self.parent_child_tensor_tree.items()}\n        return self\n</code></pre>"},{"location":"api/hierarchy/#hierarchical_loss.hierarchy.Hierarchy.to","title":"<code>to(device)</code>","text":"<p>Moves all computed tensors to the specified device.</p> Source code in <code>hierarchical_loss/hierarchy.py</code> <pre><code>def to(self, device: torch.device | str):\n    \"\"\"Moves all computed tensors to the specified device.\"\"\"\n    self.parent_tensor = self.parent_tensor.to(device)\n    self.index_tensor = self.index_tensor.to(device)\n    self.hierarchy_mask = self.hierarchy_mask.to(device)\n    self.sibling_mask = self.sibling_mask.to(device)\n    self.roots = self.roots.to(device) \n    self.parent_child_tensor_tree = {k: v.to(device) for k, v in self.parent_child_tensor_tree.items()}\n    return self\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/","title":"Utilities for working with hierarchies as tensors","text":""},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.accumulate_hierarchy","title":"<code>accumulate_hierarchy(predictions, hierarchy_index, reduce_op, identity_value)</code>","text":"<p>Performs a reduce operation along a hierarchical structure.</p> <p>This function applies a reduction operation (e.g., <code>torch.sum</code>, <code>torch.max</code>) along each ancestral path in a hierarchy. The implementation is fully vectorized. It first gathers the initial values for all nodes along each path, replaces padded values with the <code>identity_value</code>, and then applies the <code>reduce_op</code> along the path dimension.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>Tensor</code> <p>A tensor of shape <code>[B, D, N]</code>, where <code>B</code> is the batch size, <code>D</code> is the number of detections, and <code>N</code> is the number of classes.</p> required <code>hierarchy_index</code> <code>Tensor</code> <p>An int tensor of shape <code>[N, M]</code> encoding the hierarchy, where <code>N</code> is the number of classes and <code>M</code> is the maximum hierarchy depth. Each row <code>i</code> contains the path from node <code>i</code> to its root. Parent node IDs are to the right of child node IDs. A value of -1 is used for padding.</p> required <code>reduce_op</code> <code>Callable[[Tensor, int], Tensor]</code> <p>A function that performs a reduction operation along a dimension, such as <code>torch.sum</code> or <code>torch.max</code>. It must accept a tensor and a <code>dim</code> argument, and return a tensor.</p> required <code>identity_value</code> <code>float | int</code> <p>The identity value for the reduction operation. For example, <code>0.0</code> for <code>torch.sum</code> or <code>-torch.inf</code> for <code>torch.max</code>.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the same shape as <code>predictions</code> (but with the last dimension, M, reduced) containing the aggregated values along each path.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hierarchy_index = torch.tensor([\n...     [ 0,  1,  2],\n...     [ 1,  2, -1],\n...     [ 2, -1, -1],\n...     [ 3,  4, -1],\n...     [ 4, -1, -1]\n... ], dtype=torch.int64)\n&gt;&gt;&gt; # Predictions for 5 classes: [0., 10., 20., 30., 40.]\n&gt;&gt;&gt; predictions = torch.arange(0, 50, 10, dtype=torch.float32).view(1, 1, 5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 1: Hierarchical Sum\n&gt;&gt;&gt; # Path 0: [0, 1, 2] -&gt; 0. + 10. + 20. = 30.\n&gt;&gt;&gt; # Path 1: [1, 2]   -&gt; 10. + 20. = 30.\n&gt;&gt;&gt; # Path 2: [2]      -&gt; 20. = 20.\n&gt;&gt;&gt; # Path 3: [3, 4]   -&gt; 30. + 40. = 70.\n&gt;&gt;&gt; # Path 4: [4]      -&gt; 40. = 40.\n&gt;&gt;&gt; sum_preds = accumulate_hierarchy(predictions, hierarchy_index, torch.sum, 0.0)\n&gt;&gt;&gt; print(sum_preds.squeeze())\ntensor([30., 30., 20., 70., 40.])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Example 2: Hierarchical Max\n&gt;&gt;&gt; # Path 0: [0, 1, 2] -&gt; max(0., 10., 20.) = 20.\n&gt;&gt;&gt; # Path 1: [1, 2]   -&gt; max(10., 20.) = 20.\n&gt;&gt;&gt; # Path 2: [2]      -&gt; max(20.) = 20.\n&gt;&gt;&gt; # Path 3: [3, 4]   -&gt; max(30., 40.) = 40.\n&gt;&gt;&gt; # Path 4: [4]      -&gt; max(40.) = 40.\n&gt;&gt;&gt; max_op = lambda x, dim: torch.max(x, dim=dim).values\n&gt;&gt;&gt; max_preds = accumulate_hierarchy(predictions, hierarchy_index, max_op, -torch.inf)\n&gt;&gt;&gt; print(max_preds.squeeze())\ntensor([20., 20., 20., 40., 40.])\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def accumulate_hierarchy(\n    predictions: torch.Tensor,\n    hierarchy_index: torch.Tensor,\n    reduce_op: Callable[[torch.Tensor, int], torch.Tensor],\n    identity_value: float | int,\n) -&gt; torch.Tensor:\n    \"\"\"Performs a reduce operation along a hierarchical structure.\n\n    This function applies a reduction operation (e.g., `torch.sum`,\n    `torch.max`) along each ancestral path in a hierarchy. The implementation\n    is fully vectorized. It first gathers the initial values for all\n    nodes along each path, replaces padded values with the `identity_value`,\n    and then applies the `reduce_op` along the path dimension.\n\n    Parameters\n    ----------\n    predictions : torch.Tensor\n        A tensor of shape `[B, D, N]`, where `B` is the batch size, `D` is the\n        number of detections, and `N` is the number of classes.\n    hierarchy_index : torch.Tensor\n        An int tensor of shape `[N, M]` encoding the hierarchy, where `N` is the\n        number of classes and `M` is the maximum hierarchy depth. Each row `i`\n        contains the path from node `i` to its root. Parent node IDs are to\n        the right of child node IDs. A value of -1 is used for padding.\n    reduce_op : Callable[[torch.Tensor, int], torch.Tensor]\n        A function that performs a reduction operation along a dimension,\n        such as `torch.sum` or `torch.max`. It must accept a tensor\n        and a `dim` argument, and return a tensor.\n    identity_value : float | int\n        The identity value for the reduction operation. For example,\n        `0.0` for `torch.sum` or `-torch.inf` for `torch.max`.\n\n    Returns\n    -------\n    torch.Tensor\n        A new tensor with the same shape as `predictions` (but with the\n        last dimension, M, reduced) containing the aggregated values\n        along each path.\n\n    Examples\n    --------\n    &gt;&gt;&gt; hierarchy_index = torch.tensor([\n    ...     [ 0,  1,  2],\n    ...     [ 1,  2, -1],\n    ...     [ 2, -1, -1],\n    ...     [ 3,  4, -1],\n    ...     [ 4, -1, -1]\n    ... ], dtype=torch.int64)\n    &gt;&gt;&gt; # Predictions for 5 classes: [0., 10., 20., 30., 40.]\n    &gt;&gt;&gt; predictions = torch.arange(0, 50, 10, dtype=torch.float32).view(1, 1, 5)\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Example 1: Hierarchical Sum\n    &gt;&gt;&gt; # Path 0: [0, 1, 2] -&gt; 0. + 10. + 20. = 30.\n    &gt;&gt;&gt; # Path 1: [1, 2]   -&gt; 10. + 20. = 30.\n    &gt;&gt;&gt; # Path 2: [2]      -&gt; 20. = 20.\n    &gt;&gt;&gt; # Path 3: [3, 4]   -&gt; 30. + 40. = 70.\n    &gt;&gt;&gt; # Path 4: [4]      -&gt; 40. = 40.\n    &gt;&gt;&gt; sum_preds = accumulate_hierarchy(predictions, hierarchy_index, torch.sum, 0.0)\n    &gt;&gt;&gt; print(sum_preds.squeeze())\n    tensor([30., 30., 20., 70., 40.])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # Example 2: Hierarchical Max\n    &gt;&gt;&gt; # Path 0: [0, 1, 2] -&gt; max(0., 10., 20.) = 20.\n    &gt;&gt;&gt; # Path 1: [1, 2]   -&gt; max(10., 20.) = 20.\n    &gt;&gt;&gt; # Path 2: [2]      -&gt; max(20.) = 20.\n    &gt;&gt;&gt; # Path 3: [3, 4]   -&gt; max(30., 40.) = 40.\n    &gt;&gt;&gt; # Path 4: [4]      -&gt; max(40.) = 40.\n    &gt;&gt;&gt; max_op = lambda x, dim: torch.max(x, dim=dim).values\n    &gt;&gt;&gt; max_preds = accumulate_hierarchy(predictions, hierarchy_index, max_op, -torch.inf)\n    &gt;&gt;&gt; print(max_preds.squeeze())\n    tensor([20., 20., 20., 40., 40.])\n    \"\"\"\n    B, D, N = predictions.shape\n    M = hierarchy_index.shape[1]\n\n    # 1. GATHER: Collect prediction values for each node in each path.\n    # Create a mask for valid indices (non -1)\n    valid_mask = hierarchy_index != -1\n\n    # Create a \"safe\" index tensor to prevent out-of-bounds errors from -1.\n    # We replace -1 with a valid index (e.g., 0) and will zero out its\n    # contribution later using the mask.\n    safe_index = hierarchy_index.masked_fill(~valid_mask, 0)\n\n    # Use advanced indexing to gather values. `predictions[:, :, safe_index]`\n    # creates a tensor of shape [B, D, N, M].\n    path_values = predictions[:, :, safe_index]\n\n    # Replace the invalid, padded values with the appropriate identity value.\n    # The valid_mask broadcasts from [N, M] to [B, D, N, M].\n    path_values = torch.where(\n        valid_mask,\n        path_values,\n        identity_value\n    )\n\n    # 2. ACCUMULATE: Apply the reduction operation along the path dimension.\n    final_values = reduce_op(path_values, -1)\n\n    return final_values\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.build_hierarchy_index_tensor","title":"<code>build_hierarchy_index_tensor(hierarchy, device=None)</code>","text":"<p>Creates a 2D tensor mapping each node to its full ancestor path.</p> <p>This function translates a {child: parent} dictionary hierarchy into a 2D tensor. The hierarchy MUST BE DENSE, in the sense that the keys and values must run from 0 to C-1 where C is the number of nodes. Each row <code>i</code> of the tensor corresponds to node <code>i</code>. The row contains the full ancestor path starting with the node itself: <code>[node_id, parent_id, grandparent_id, ..., root_id]</code>.</p> <p>The paths are right-padded with -1 to the length of the longest ancestor path in the hierarchy.</p> <p>This tensor is used as an index for hierarchical accumulation operations.</p> <p>Parameters:</p> Name Type Description Default <code>hierarchy</code> <code>dict[int, int]</code> <p>A tree in {child: parent} format. Node IDs must be non-negative integers that can be used as tensor indices.</p> required <code>device</code> <code>device | str | None</code> <p>The desired device for the output tensor. If <code>None</code>, uses the default PyTorch device. By default <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 2D tensor of shape <code>(C, M)</code>, where <code>M</code> is the maximum hierarchy depth and <code>C</code> is the number of categories (nodes in the tree). <code>tensor[i]</code> contains the ancestor path for node <code>i</code>, padded with -1.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hierarchy = {0: 1, 1: 2, 3: 4}\n&gt;&gt;&gt; # Nodes found: {0, 1, 2, 3, 4} -&gt; len=5\n&gt;&gt;&gt; # Max depth: 3 (for node 0)\n&gt;&gt;&gt; build_hierarchy_index_tensor(hierarchy)\ntensor([[ 0,  1,  2],\n        [ 1,  2, -1],\n        [ 2, -1, -1],\n        [ 3,  4, -1],\n        [ 4, -1, -1]], dtype=torch.int32)\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def build_hierarchy_index_tensor(\n    hierarchy: dict[int, int], device: torch.device | str | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Creates a 2D tensor mapping each node to its full ancestor path.\n\n    This function translates a {child: parent} dictionary hierarchy into a\n    2D tensor. The hierarchy MUST BE DENSE, in the sense that the keys and values\n    must run from 0 to C-1 where C is the number of nodes.\n    Each row `i` of the tensor corresponds to node `i`. The\n    row contains the full ancestor path starting with the node itself:\n    `[node_id, parent_id, grandparent_id, ..., root_id]`.\n\n    The paths are right-padded with -1 to the length of the longest\n    ancestor path in the hierarchy.\n\n    This tensor is used as an index for hierarchical accumulation operations.\n\n    Parameters\n    ----------\n    hierarchy : dict[int, int]\n        A tree in {child: parent} format. Node IDs must be non-negative\n        integers that can be used as tensor indices.\n    device : torch.device | str | None, optional\n        The desired device for the output tensor. If `None`, uses the\n        default PyTorch device. By default `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        A 2D tensor of shape `(C, M)`, where `M` is the maximum hierarchy\n        depth and `C` is the number of categories (nodes in the tree).\n        `tensor[i]` contains the ancestor path for node `i`, padded with -1.\n\n    Examples\n    --------\n    &gt;&gt;&gt; hierarchy = {0: 1, 1: 2, 3: 4}\n    &gt;&gt;&gt; # Nodes found: {0, 1, 2, 3, 4} -&gt; len=5\n    &gt;&gt;&gt; # Max depth: 3 (for node 0)\n    &gt;&gt;&gt; build_hierarchy_index_tensor(hierarchy)\n    tensor([[ 0,  1,  2],\n            [ 1,  2, -1],\n            [ 2, -1, -1],\n            [ 3,  4, -1],\n            [ 4, -1, -1]], dtype=torch.int32)\n    \"\"\"\n    lens = get_ancestor_chain_lens(hierarchy)\n    index_tensor = torch.full((len(lens), max(lens.values())), -1, dtype=torch.int32, device=device)\n    preorder_apply(hierarchy, set_indices, index_tensor)\n    return index_tensor\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.build_hierarchy_sibling_mask","title":"<code>build_hierarchy_sibling_mask(parent_tensor, device=None)</code>","text":"<p>Creates a boolean mask identifying sibling groups from a parent tensor.</p> <p>This function is used to prepare a mask for <code>utils.logsumexp_over_siblings</code>. It takes the 1D parent tensor (where <code>parent_tensor[i] = parent_id</code>) and creates a 2D mask.</p> <p>Each column <code>g</code> in the mask represents a unique sibling group (i.e., a unique parent, including -1 for the root group). A node <code>i</code> will have <code>True</code> in column <code>g</code> if its parent is the parent corresponding to sibling group <code>g</code>.</p> <p>Parameters:</p> Name Type Description Default <code>parent_tensor</code> <code>Tensor</code> <p>A 1D tensor of shape <code>(C,)</code>, where <code>C</code> is the number of classes. <code>parent_tensor[i]</code> contains the integer ID of the parent of node <code>i</code>, or -1 for root nodes.  See the <code>build_parent_tensor</code> function.</p> required <code>device</code> <code>device | str | None</code> <p>The desired device for the output tensor. If <code>None</code>, uses the default PyTorch device. By default <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 2D boolean tensor of shape <code>(C, G)</code>, where <code>G</code> is the number of unique parent groups (including roots). <code>mask[i, g]</code> is <code>True</code> if node <code>i</code> belongs to sibling group <code>g</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Node parents: 0-&gt;1, 1-&gt;2, 2-&gt;-1, 3-&gt;2, 4-&gt;-1, 5-&gt;6, 6-&gt;-1\n&gt;&gt;&gt; parent_tensor = torch.tensor([ 1,  2, -1,  2, -1,  6, -1])\n&gt;&gt;&gt; # Unique parents (groups): -1, 1, 2, 6\n&gt;&gt;&gt; build_hierarchy_sibling_mask(parent_tensor)\ntensor([[False,  True, False, False],\n        [False, False,  True, False],\n        [ True, False, False, False],\n        [False, False,  True, False],\n        [ True, False, False, False],\n        [False, False, False,  True],\n        [ True, False, False, False]])\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def build_hierarchy_sibling_mask(\n    parent_tensor: torch.Tensor, device: torch.device | str | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Creates a boolean mask identifying sibling groups from a parent tensor.\n\n    This function is used to prepare a mask for `utils.logsumexp_over_siblings`.\n    It takes the 1D parent tensor (where `parent_tensor[i] = parent_id`)\n    and creates a 2D mask.\n\n    Each column `g` in the mask represents a unique sibling group (i.e., a\n    unique parent, including -1 for the root group). A node `i` will have\n    `True` in column `g` if its parent is the parent corresponding to\n    sibling group `g`.\n\n    Parameters\n    ----------\n    parent_tensor : torch.Tensor\n        A 1D tensor of shape `(C,)`, where `C` is the number of classes.\n        `parent_tensor[i]` contains the integer ID of the parent of node `i`,\n        or -1 for root nodes.  See the `build_parent_tensor` function.\n    device : torch.device | str | None, optional\n        The desired device for the output tensor. If `None`, uses the\n        default PyTorch device. By default `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        A 2D boolean tensor of shape `(C, G)`, where `G` is the number of\n        unique parent groups (including roots). `mask[i, g]` is `True`\n        if node `i` belongs to sibling group `g`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Node parents: 0-&gt;1, 1-&gt;2, 2-&gt;-1, 3-&gt;2, 4-&gt;-1, 5-&gt;6, 6-&gt;-1\n    &gt;&gt;&gt; parent_tensor = torch.tensor([ 1,  2, -1,  2, -1,  6, -1])\n    &gt;&gt;&gt; # Unique parents (groups): -1, 1, 2, 6\n    &gt;&gt;&gt; build_hierarchy_sibling_mask(parent_tensor)\n    tensor([[False,  True, False, False],\n            [False, False,  True, False],\n            [ True, False, False, False],\n            [False, False,  True, False],\n            [ True, False, False, False],\n            [False, False, False,  True],\n            [ True, False, False, False]])\n    \"\"\"\n    C = parent_tensor.shape[0]\n\n    # Identify all unique parents (which uniquely define child groups), including -1 for roots\n    unique_parents, inverse_indices = torch.unique(parent_tensor, return_inverse=True)\n    G = len(unique_parents)\n\n    sibling_mask = torch.zeros(C, G, dtype=torch.bool, device=device)\n\n    # Assign each node to the column of its parent group\n    sibling_mask[torch.arange(C), inverse_indices] = True\n\n    return sibling_mask\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.build_parent_tensor","title":"<code>build_parent_tensor(tree, device=None)</code>","text":"<p>Converts a {child: parent} dictionary tree into a 1D parent tensor.</p> <p>This function creates a 1D tensor where the value at each index <code>i</code> is the ID of that node's parent. Nodes that are not children (i.e., roots) will have a value of -1.</p> <p>The size of the tensor is determined by the maximum node ID present in the tree (in either keys or values).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[int, int]</code> <p>A tree in {child: parent} format. Node IDs are assumed to be non-negative integers.</p> required <code>device</code> <code>device | str | None</code> <p>The desired device for the output tensor. If <code>None</code>, uses the default PyTorch device. By default <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A 1D tensor of shape <code>(C,)</code>, where <code>C</code> is <code>max(all_node_ids) + 1</code>. <code>parent_tensor[i]</code> contains the ID of the parent of node <code>i</code>, or -1 if <code>i</code> is a root or not in the hierarchy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n&gt;&gt;&gt; # Max node ID is 6, so tensor size is 7\n&gt;&gt;&gt; build_parent_tensor(tree)\ntensor([ 1,  2, -1,  2, -1,  6, -1])\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def build_parent_tensor(\n    tree: dict[int, int], device: torch.device | str | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Converts a {child: parent} dictionary tree into a 1D parent tensor.\n\n    This function creates a 1D tensor where the value at each index `i`\n    is the ID of that node's parent. Nodes that are not children (i.e.,\n    roots) will have a value of -1.\n\n    The size of the tensor is determined by the maximum node ID present\n    in the tree (in either keys or values).\n\n    Parameters\n    ----------\n    tree : dict[int, int]\n        A tree in {child: parent} format. Node IDs are assumed to be\n        non-negative integers.\n    device : torch.device | str | None, optional\n        The desired device for the output tensor. If `None`, uses the\n        default PyTorch device. By default `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        A 1D tensor of shape `(C,)`, where `C` is `max(all_node_ids) + 1`.\n        `parent_tensor[i]` contains the ID of the parent of node `i`,\n        or -1 if `i` is a root or not in the hierarchy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n    &gt;&gt;&gt; # Max node ID is 6, so tensor size is 7\n    &gt;&gt;&gt; build_parent_tensor(tree)\n    tensor([ 1,  2, -1,  2, -1,  6, -1])\n    \"\"\"\n    nodes = set(tree.keys()) | set(tree.values())\n    C = max(nodes) + 1\n\n    parent_tensor = torch.full((C,), -1, dtype=torch.long, device=device)\n\n    for child, parent in tree.items():\n        parent_tensor[child] = parent\n\n    return parent_tensor\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.expand_target_hierarchy","title":"<code>expand_target_hierarchy(target, hierarchy_index)</code>","text":"<p>Expands a one-hot target tensor up the hierarchy.</p> <p>This function takes a target tensor that is \"one-hot\" along the class dimension (i.e., contains a single non-zero value) and propagates that value to all ancestors of the target class. The implementation is fully vectorized.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Tensor</code> <p>A tensor of shape <code>[B, D, N]</code>, where <code>B</code> is the batch size, <code>D</code> is the number of detections, and <code>N</code> is the number of classes. It is assumed to be one-hot along the last dimension.</p> required <code>hierarchy_index</code> <code>Tensor</code> <p>An int tensor of shape <code>[N, M]</code> encoding the hierarchy, where <code>N</code> is the number of classes and <code>M</code> is the maximum hierarchy depth. Each row <code>i</code> contains the path from node <code>i</code> to its root.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A new tensor with the same shape as <code>target</code> where the target value has been propagated up the hierarchy.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; hierarchy_index = torch.tensor([\n...     [ 0,  1,  2],\n...     [ 1,  2, -1],\n...     [ 2, -1, -1],\n...     [ 3,  4, -1],\n...     [ 4, -1, -1]\n... ], dtype=torch.int64)\n&gt;&gt;&gt; # Target is one-hot at index 0\n&gt;&gt;&gt; target = torch.tensor([0.4, 0., 0., 0., 0.]).view(1, 1, 5)\n&gt;&gt;&gt; expanded_target = expand_target_hierarchy(target, hierarchy_index)\n&gt;&gt;&gt; print(expanded_target.squeeze())\ntensor([0.4000, 0.4000, 0.4000, 0.0000, 0.0000])\n&gt;&gt;&gt; target = torch.tensor([0., 0., 0., 0.3, 0.]).view(1, 1, 5)\n&gt;&gt;&gt; expanded_target = expand_target_hierarchy(target, hierarchy_index)\n&gt;&gt;&gt; print(expanded_target.squeeze())\ntensor([0.0000, 0.0000, 0.0000, 0.3000, 0.3000])\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def expand_target_hierarchy(\n    target: torch.Tensor, hierarchy_index: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Expands a one-hot target tensor up the hierarchy.\n\n    This function takes a target tensor that is \"one-hot\" along the class\n    dimension (i.e., contains a single non-zero value) and propagates that\n    value to all ancestors of the target class. The implementation is fully\n    vectorized.\n\n    Parameters\n    ----------\n    target : torch.Tensor\n        A tensor of shape `[B, D, N]`, where `B` is the batch size, `D` is the\n        number of detections, and `N` is the number of classes. It is assumed\n        to be one-hot along the last dimension.\n    hierarchy_index : torch.Tensor\n        An int tensor of shape `[N, M]` encoding the hierarchy, where `N` is the\n        number of classes and `M` is the maximum hierarchy depth. Each row `i`\n        contains the path from node `i` to its root.\n\n    Returns\n    -------\n    torch.Tensor\n        A new tensor with the same shape as `target` where the target value has\n        been propagated up the hierarchy.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; hierarchy_index = torch.tensor([\n    ...     [ 0,  1,  2],\n    ...     [ 1,  2, -1],\n    ...     [ 2, -1, -1],\n    ...     [ 3,  4, -1],\n    ...     [ 4, -1, -1]\n    ... ], dtype=torch.int64)\n    &gt;&gt;&gt; # Target is one-hot at index 0\n    &gt;&gt;&gt; target = torch.tensor([0.4, 0., 0., 0., 0.]).view(1, 1, 5)\n    &gt;&gt;&gt; expanded_target = expand_target_hierarchy(target, hierarchy_index)\n    &gt;&gt;&gt; print(expanded_target.squeeze())\n    tensor([0.4000, 0.4000, 0.4000, 0.0000, 0.0000])\n    &gt;&gt;&gt; target = torch.tensor([0., 0., 0., 0.3, 0.]).view(1, 1, 5)\n    &gt;&gt;&gt; expanded_target = expand_target_hierarchy(target, hierarchy_index)\n    &gt;&gt;&gt; print(expanded_target.squeeze())\n    tensor([0.0000, 0.0000, 0.0000, 0.3000, 0.3000])\n    \"\"\"\n    M = hierarchy_index.shape[1]\n\n    # Find the single non-zero value and its index in the target tensor.\n    hot_value, hot_index = torch.max(target, dim=-1)\n\n    # Gather the ancestral paths corresponding to the hot indices.\n    # The shape will be [B, D, M].\n    paths = hierarchy_index[hot_index]\n\n    # Create a mask for valid indices (non -1) to handle padded paths.\n    valid_mask = paths != -1\n\n    # Create a \"safe\" index tensor to prevent out-of-bounds errors from -1.\n    # We replace -1 with a valid index (e.g., 0) and will zero out its\n    # contribution later using a masked source.\n    safe_paths = paths.masked_fill(~valid_mask, 0)\n    safe_paths_ints = safe_paths.to(torch.int64)\n\n    # Prepare the source tensor for the scatter operation.\n    # It should have the same value (`hot_value`) for all valid path members.\n    src_values = hot_value.unsqueeze(-1).expand(-1, -1, M)\n    masked_src = src_values * valid_mask.to(src_values.dtype)\n\n    # Create an output tensor and scatter the hot value into all ancestral positions.\n    expanded_target = torch.zeros_like(target)\n    expanded_target.scatter_(dim=-1, index=safe_paths_ints, src=masked_src)\n\n    return expanded_target\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.find_closest_permitted_parent","title":"<code>find_closest_permitted_parent(node, tree, permitted_nodes)</code>","text":"<p>Finds the first ancestor of a node that is in a permitted set.</p> <p>This function walks up the ancestral chain of a node (using the {child: parent} tree) and returns the first ancestor it finds that is present in the <code>permitted_nodes</code> set.</p> <p>If no ancestor (including the node itself) is in the set, or if the node is not in the tree to begin with, it returns None.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Hashable</code> <p>The ID of the node to start searching from.</p> required <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs that are considered \"permitted\".</p> required <p>Returns:</p> Type Description <code>Hashable | None</code> <p>The ID of the closest permitted ancestor, or None if none is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n&gt;&gt;&gt; permitted = {0, 2, 5}\n&gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n&gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n&gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def find_closest_permitted_parent(\n    node: Hashable,\n    tree: dict[Hashable, Hashable],\n    permitted_nodes: set[Hashable],\n) -&gt; Hashable | None:\n    \"\"\"Finds the first ancestor of a node that is in a permitted set.\n\n    This function walks up the ancestral chain of a node (using the\n    {child: parent} tree) and returns the first ancestor it finds\n    that is present in the `permitted_nodes` set.\n\n    If no ancestor (including the node itself) is in the set,\n    or if the node is not in the tree to begin with, it returns None.\n\n    Parameters\n    ----------\n    node : Hashable\n        The ID of the node to start searching from.\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs that are considered \"permitted\".\n\n    Returns\n    -------\n    Hashable | None\n        The ID of the closest permitted ancestor, or None if none is found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n    &gt;&gt;&gt; permitted = {0, 2, 5}\n    &gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n    &gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n    &gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n    \"\"\"\n    if node not in tree:\n        return None\n    parent = tree[node]\n    while parent not in permitted_nodes:\n        if parent in tree:\n            parent = tree[parent]\n        else:\n            return None\n    return parent\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.get_ancestor_chain_lens","title":"<code>get_ancestor_chain_lens(tree)</code>","text":"<p>Get lengths of ancestor chains in a { child: parent } dictionary tree</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n{3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in { child: parent } format.</p> required <p>Returns:</p> Name Type Description <code>lengths</code> <code>dict[Hashable, int]</code> <p>The lengths of the path to the root from each node { node: length }</p> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_ancestor_chain_lens(tree: dict[Hashable, Hashable]) -&gt; dict[Hashable, int]:\n    '''\n    Get lengths of ancestor chains in a { child: parent } dictionary tree\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n    {3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n\n    Parameters\n    ----------\n    tree: dict[Hashable, Hashable]\n        A tree in { child: parent } format.\n\n    Returns\n    -------\n    lengths: dict[Hashable, int]\n        The lengths of the path to the root from each node { node: length }\n\n    '''\n    return preorder_apply(tree, _increment_chain_len)\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.get_roots","title":"<code>get_roots(tree)</code>","text":"<p>Finds all root nodes in a {child: parent} tree.</p> <p>A root node is defined as any node that is not a child of another node in the tree (i.e., its ancestor chain length is 1).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>A list of all root nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n&gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n[2, 6]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_roots(tree: dict[Hashable, Hashable]) -&gt; list[Hashable]:\n    \"\"\"Finds all root nodes in a {child: parent} tree.\n\n    A root node is defined as any node that is not a child of another\n    node in the tree (i.e., its ancestor chain length is 1).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    list[Hashable]\n        A list of all root nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n    &gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n    [2, 6]\n    \"\"\"\n    ancestor_chain_lens = get_ancestor_chain_lens(tree)\n    return [node for node in ancestor_chain_lens if ancestor_chain_lens[node] == 1]\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.hierarchically_index_flat_scores","title":"<code>hierarchically_index_flat_scores(flat_scores, target_indices, hierarchy_index_tensor, hierarchy_mask, device=None)</code>","text":"<p>Gathers scores from a flat score tensor along specified hierarchical paths.</p> <p>This function takes a batch of flat scores (<code>B, P, C</code>) and a batch of target category indices (<code>B, P</code>). For each target index, it looks up the full ancestral path in <code>hierarchy_index_tensor</code> (<code>C, H</code>) and gathers the corresponding scores from <code>flat_scores</code>.</p> <p>It then applies the <code>hierarchy_mask</code> to the gathered scores, zeroing out entries where the mask is <code>True</code>.</p> <p>Parameters:</p> Name Type Description Default <code>flat_scores</code> <code>Tensor</code> <p>A tensor of flat scores with shape <code>(B, P, C)</code>, where <code>B</code> is batch size, <code>P</code> is number of proposals, and <code>C</code> is number of categories.</p> required <code>target_indices</code> <code>Tensor</code> <p>A long tensor of shape <code>(B, P)</code> containing the leaf category index for each proposal.</p> required <code>hierarchy_index_tensor</code> <code>Tensor</code> <p>A long tensor of shape <code>(C, H)</code> mapping each category <code>c</code> to its ancestral path. <code>H</code> is the max hierarchy depth.</p> required <code>hierarchy_mask</code> <code>Tensor</code> <p>A boolean invalidity mask of shape <code>(C, H)</code>. <code>True</code> indicates an invalid entry (e.g., padding) that should be zeroed out. This mask is indexed by <code>target_indices</code> and applied to the gathered scores.</p> required <code>device</code> <code>device | str | None</code> <p>The desired device for <code>torch.arange</code>. If <code>None</code>, uses the default PyTorch device. By default <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>A tensor of shape <code>(B, P, H)</code> containing the gathered scores along each target's ancestral path, after masking.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; # B=1, P=1, C=2\n&gt;&gt;&gt; flat_scores = torch.tensor([[[10., 20.]]])\n&gt;&gt;&gt; # Target index is 0\n&gt;&gt;&gt; target_indices = torch.tensor([[0]])\n&gt;&gt;&gt; # C=2, H=3. Path 0 is [0, 1, -1]\n&gt;&gt;&gt; hierarchy_index_tensor = torch.tensor([[0, 1, -1], [1, -1, -1]], dtype=torch.long)\n&gt;&gt;&gt; # Create an invalidity mask (True where path is -1)\n&gt;&gt;&gt; invalidity_mask = (hierarchy_index_tensor == -1)\n&gt;&gt;&gt; print(invalidity_mask)\ntensor([[False, False,  True],\n        [False,  True,  True]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # The function will gather scores for path [0, 1, -1] -&gt; [10., 20., 10.]\n&gt;&gt;&gt; # (Note: -1 safely indexes 0)\n&gt;&gt;&gt; # It will apply the mask for index 0: [False, False, True]\n&gt;&gt;&gt; # Result: [10., 20., 0.]\n&gt;&gt;&gt; hierarchically_index_flat_scores(\n...     flat_scores, target_indices, hierarchy_index_tensor, invalidity_mask\n... )\ntensor([[[10., 20.,  0.]]])\n</code></pre> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def hierarchically_index_flat_scores(\n    flat_scores: torch.Tensor,\n    target_indices: torch.Tensor,\n    hierarchy_index_tensor: torch.Tensor,\n    hierarchy_mask: torch.Tensor,\n    device: torch.device | str | None = None,\n) -&gt; torch.Tensor:\n    \"\"\"Gathers scores from a flat score tensor along specified hierarchical paths.\n\n    This function takes a batch of flat scores (`B, P, C`) and a batch of\n    target category indices (`B, P`). For each target index, it looks up\n    the full ancestral path in `hierarchy_index_tensor` (`C, H`) and\n    gathers the corresponding scores from `flat_scores`.\n\n    It then applies the `hierarchy_mask` to the gathered scores, zeroing\n    out entries where the mask is `True`.\n\n    Parameters\n    ----------\n    flat_scores : torch.Tensor\n        A tensor of flat scores with shape `(B, P, C)`, where `B` is\n        batch size, `P` is number of proposals, and `C` is number\n        of categories.\n    target_indices : torch.Tensor\n        A long tensor of shape `(B, P)` containing the leaf category\n        index for each proposal.\n    hierarchy_index_tensor : torch.Tensor\n        A long tensor of shape `(C, H)` mapping each category `c` to its\n        ancestral path. `H` is the max hierarchy depth.\n    hierarchy_mask : torch.Tensor\n        A boolean **invalidity** mask of shape `(C, H)`. `True` indicates\n        an invalid entry (e.g., padding) that should be zeroed out.\n        This mask is indexed by `target_indices` and applied to the\n        gathered scores.\n    device : torch.device | str | None, optional\n        The desired device for `torch.arange`. If `None`, uses the\n        default PyTorch device. By default `None`.\n\n    Returns\n    -------\n    torch.Tensor\n        A tensor of shape `(B, P, H)` containing the gathered scores\n        along each target's ancestral path, after masking.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; # B=1, P=1, C=2\n    &gt;&gt;&gt; flat_scores = torch.tensor([[[10., 20.]]])\n    &gt;&gt;&gt; # Target index is 0\n    &gt;&gt;&gt; target_indices = torch.tensor([[0]])\n    &gt;&gt;&gt; # C=2, H=3. Path 0 is [0, 1, -1]\n    &gt;&gt;&gt; hierarchy_index_tensor = torch.tensor([[0, 1, -1], [1, -1, -1]], dtype=torch.long)\n    &gt;&gt;&gt; # Create an invalidity mask (True where path is -1)\n    &gt;&gt;&gt; invalidity_mask = (hierarchy_index_tensor == -1)\n    &gt;&gt;&gt; print(invalidity_mask)\n    tensor([[False, False,  True],\n            [False,  True,  True]])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; # The function will gather scores for path [0, 1, -1] -&gt; [10., 20., 10.]\n    &gt;&gt;&gt; # (Note: -1 safely indexes 0)\n    &gt;&gt;&gt; # It will apply the mask for index 0: [False, False, True]\n    &gt;&gt;&gt; # Result: [10., 20., 0.]\n    &gt;&gt;&gt; hierarchically_index_flat_scores(\n    ...     flat_scores, target_indices, hierarchy_index_tensor, invalidity_mask\n    ... )\n    tensor([[[10., 20.,  0.]]])\n    \"\"\"\n    batch_size, n_proposals, n_categories = flat_scores.shape\n    hierarchy_size = hierarchy_index_tensor.shape[1]\n\n    hierarchy_indices = hierarchy_index_tensor[target_indices]\n    flat_mask = hierarchy_mask[target_indices]\n\n    # Construct batch indices\n    batch_indices = torch.arange(batch_size, device=device).view(batch_size, 1, 1).expand(batch_size, n_proposals, hierarchy_size) # (B, N, H)\n    proposal_indices = torch.arange(n_proposals, device=device).view(1, n_proposals, 1).expand(batch_size, n_proposals, hierarchy_size) # (B, N, H)\n\n    gathered_scores = flat_scores[batch_indices, proposal_indices, hierarchy_indices]  # (B, N, H)\n\n    # Mask out invalid entries\n    masked_scores = gathered_scores.masked_fill(flat_mask, 0.)\n\n    return masked_scores\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.invert_childparent_tree","title":"<code>invert_childparent_tree(tree)</code>","text":"<p>Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.</p> <p>This function inverts the standard {child: parent} structure, creating a nested dictionary that starts from the root(s). It uses <code>preorder_apply</code> to traverse the tree top-down and build the nested structure.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the tree in a top-down format, e.g., <code>{root: {child: {grandchild: {}}}}</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n&gt;&gt;&gt; invert_childparent_tree(tree)\n{2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def invert_childparent_tree(tree: dict[Hashable, Hashable]) -&gt; dict:\n    \"\"\"Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.\n\n    This function inverts the standard {child: parent} structure, creating\n    a nested dictionary that starts from the root(s). It uses\n    `preorder_apply` to traverse the tree top-down and build the\n    nested structure.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree in a top-down format,\n        e.g., `{root: {child: {grandchild: {}}}}`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n    &gt;&gt;&gt; invert_childparent_tree(tree)\n    {2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n    \"\"\"\n    parentchild_tree = {}\n    preorder_apply(tree, _append_to_parentchild_tree, parentchild_tree)\n    return parentchild_tree\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.preorder_apply","title":"<code>preorder_apply(tree, f, *args)</code>","text":"<p>Applies a function to all nodes in a tree in a pre-order (top-down) fashion.</p> <p>This function works by first finding an ancestor path (from leaf to root). It then applies the function <code>f</code> to the root (or highest unvisited node) and iterates down the path, applying <code>f</code> to each child and passing in the result from its parent. This top-down application is a pre-order traversal.</p> <p>It uses memoization (the <code>visited</code> dict) to ensure that <code>f</code> is applied to each node only once, even in multi-branch trees.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>f</code> <code>Callable</code> <p>The function to apply to each node. Its signature must be <code>f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any</code>.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments to be passed to every call of <code>f</code>.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[Hashable, Any]</code> <p>A dictionary mapping each node ID to the result of <code>f(node, ...)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n&gt;&gt;&gt; def f(node, parent_depth):\n...     # parent_depth is the result from the parent node\n...     return 1 if parent_depth is None else parent_depth + 1\n...\n&gt;&gt;&gt; preorder_apply(tree, f)\n{2: 1, 1: 2, 0: 3, 3: 2}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def preorder_apply(tree: dict[Hashable, Hashable], f: Callable, *args: Any) -&gt; dict[Hashable, Any]:\n    \"\"\"Applies a function to all nodes in a tree in a pre-order (top-down) fashion.\n\n    This function works by first finding an ancestor path (from leaf to root).\n    It then applies the function `f` to the root (or highest unvisited node)\n    and iterates *down* the path, applying `f` to each child and passing in\n    the result from its parent. This top-down application is a pre-order\n    traversal.\n\n    It uses memoization (the `visited` dict) to ensure that `f` is\n    applied to each node only once, even in multi-branch trees.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    f : Callable\n        The function to apply to each node. Its signature must be\n        `f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any`.\n    *args: Any\n        Additional positional arguments to be passed to every call of `f`.\n\n    Returns\n    -------\n    dict[Hashable, Any]\n        A dictionary mapping each node ID to the result of `f(node, ...)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n    &gt;&gt;&gt; def f(node, parent_depth):\n    ...     # parent_depth is the result from the parent node\n    ...     return 1 if parent_depth is None else parent_depth + 1\n    ...\n    &gt;&gt;&gt; preorder_apply(tree, f)\n    {2: 1, 1: 2, 0: 3, 3: 2}\n    \"\"\"\n    visited = {}\n    for node in tree:\n        path = [node]\n        while (node in tree) and (node not in visited):\n            node = tree[node]\n            path.append(node)\n        if node not in visited:\n            visited[node] = f(node, None, *args)\n        for i in range(-2, -len(path) - 1, -1):\n            visited[path[i]] = f(path[i], visited[path[i+1]], *args)\n    return visited\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.set_indices","title":"<code>set_indices(index, parent_index, tensor)</code>","text":"<p>A helper function for <code>preorder_apply</code> to build an ancestor path tensor.</p> <p>This function populates a single row of the <code>tensor</code> (the row specified by <code>index</code>). It sets the first element of the row to <code>index</code> itself. If a <code>parent_index</code> is provided, it copies the parent's ancestor path (its row, excluding the last element) into the current node's row (starting from the second element).</p> <p>This creates the desired row format: [node_id, parent_id, grand_parent_id, ...].</p> <p>It is designed to be used with <code>tree_utils.preorder_apply</code>, where: - <code>index</code> is the <code>node</code> - <code>parent_index</code> is the <code>parent_result</code> (the return value from the   parent's call, which is the parent's index) - <code>tensor</code> is the <code>*args</code></p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>The node ID, which corresponds to the row index in the tensor.</p> required <code>parent_index</code> <code>int or None</code> <p>The ID of the parent node, or None if the node is a root.</p> required <code>tensor</code> <code>Tensor</code> <p>The 2D tensor being populated with ancestor paths.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The <code>index</code> of the current node, to be passed as the <code>parent_index</code> to its children during the pre-order traversal.</p> Source code in <code>hierarchical_loss/hierarchy_tensor_utils.py</code> <pre><code>def set_indices(index: int, parent_index: int | None, tensor: torch.Tensor) -&gt; int:\n    \"\"\"A helper function for `preorder_apply` to build an ancestor path tensor.\n\n    This function populates a single row of the `tensor` (the row specified\n    by `index`). It sets the first element of the row to `index` itself.\n    If a `parent_index` is provided, it copies the parent's ancestor path\n    (its row, excluding the last element) into the current node's row\n    (starting from the second element).\n\n    This creates the desired row format: [node_id, parent_id, grand_parent_id, ...].\n\n    It is designed to be used with `tree_utils.preorder_apply`, where:\n    - `index` is the `node`\n    - `parent_index` is the `parent_result` (the return value from the\n      parent's call, which is the parent's index)\n    - `tensor` is the `*args`\n\n    Parameters\n    ----------\n    index : int\n        The node ID, which corresponds to the row index in the tensor.\n    parent_index : int or None\n        The ID of the parent node, or None if the node is a root.\n    tensor : torch.Tensor\n        The 2D tensor being populated with ancestor paths.\n\n    Returns\n    -------\n    int\n        The `index` of the current node, to be passed as the\n        `parent_index` to its children during the pre-order traversal.\n    \"\"\"\n    tensor[index, 0] = index\n    if parent_index is not None:\n        tensor[index, 1:] = tensor[parent_index, :-1]\n    return index\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.tree_walk","title":"<code>tree_walk(tree, node)</code>","text":"<p>Walks up the ancestor chain from a starting node.</p> <p>This generator yields the starting node first, then its parent, its grandparent, and so on, until a root (a node not present as a key in the tree) is reached.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>node</code> <code>Hashable</code> <p>The node to start the walk from.</p> required <p>Yields:</p> Type Description <code>Iterator[Hashable]</code> <p>An iterator of node IDs in the ancestor chain, starting with the given node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n&gt;&gt;&gt; list(tree_walk(tree, 0))\n[0, 1, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 3))\n[3, 4, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 2))\n[2]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def tree_walk(tree: dict[Hashable, Hashable], node: Hashable) -&gt; Iterator[Hashable]:\n    \"\"\"Walks up the ancestor chain from a starting node.\n\n    This generator yields the starting node first, then its parent,\n    its grandparent, and so on, until a root (a node not\n    present as a key in the tree) is reached.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    node : Hashable\n        The node to start the walk from.\n\n    Yields\n    ------\n    Iterator[Hashable]\n        An iterator of node IDs in the ancestor chain, starting\n        with the given node.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n    &gt;&gt;&gt; list(tree_walk(tree, 0))\n    [0, 1, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 3))\n    [3, 4, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 2))\n    [2]\n    \"\"\"\n    yield node\n    while node in tree:\n        node = tree[node]\n        yield node\n</code></pre>"},{"location":"api/hierarchy_tensor_utils/#hierarchical_loss.hierarchy_tensor_utils.trim_childparent_tree","title":"<code>trim_childparent_tree(tree, permitted_nodes)</code>","text":"<p>Trims a {child: parent} tree to only include permitted nodes.</p> <p>This function first remaps every node in the tree to its closest permitted ancestor. It then filters this map, keeping only the entries where the node (the key) is also in the <code>permitted_nodes</code> set.</p> <p>The result is a new {child: parent} tree containing only permitted nodes, mapped to their closest permitted ancestor (which will be another permitted node or None).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs to keep.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Hashable | None]</code> <p>A new {child: parent} tree containing only permitted nodes, each re-mapped to its closest permitted ancestor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n&gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n&gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n{0: 2, 2: 5}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def trim_childparent_tree(\n    tree: dict[Hashable, Hashable], permitted_nodes: set[Hashable]\n) -&gt; dict[Hashable, Hashable | None]:\n    \"\"\"Trims a {child: parent} tree to only include permitted nodes.\n\n    This function first remaps every node in the tree to its closest\n    permitted ancestor. It then filters this map, keeping only the\n    entries where the node (the key) is *also* in the `permitted_nodes`\n    set.\n\n    The result is a new {child: parent} tree containing *only*\n    permitted nodes, mapped to their closest permitted ancestor\n    (which will be another permitted node or None).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs to keep.\n\n    Returns\n    -------\n    dict[Hashable, Hashable | None]\n        A new {child: parent} tree containing only permitted nodes,\n        each re-mapped to its closest permitted ancestor.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n    &gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n    &gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n    {0: 2, 2: 5}\n    \"\"\"\n    new_tree = {}\n    for node in tree:\n        closest_permitted_parent = find_closest_permitted_parent(node, tree, permitted_nodes)\n        new_tree[node] = closest_permitted_parent\n    for node in list(new_tree.keys()):\n        if new_tree[node] is None or (node not in permitted_nodes):\n            del new_tree[node]\n    return new_tree\n</code></pre>"},{"location":"api/path_utils/","title":"Utilities for working with paths through a hierarchy","text":""},{"location":"api/path_utils/#hierarchical_loss.path_utils.batch_filter_empty_paths","title":"<code>batch_filter_empty_paths(predicted_boxes, predicted_paths, predicted_path_scores)</code>","text":"<p>Applies empty path filtering to a batch of predictions.</p> <p>This function maps the <code>filter_empty_paths</code> function over a batch of predicted boxes, paths, and scores.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_boxes</code> <code>list[Tensor]</code> <p>A batch of bounding box tensors.</p> required <code>predicted_paths</code> <code>list[list[list[int]]]</code> <p>A batch of predicted path lists.</p> required <code>predicted_path_scores</code> <code>list[list[Tensor]]</code> <p>A batch of predicted path score lists.</p> required <p>Returns:</p> Type Description <code>list[tuple[Tensor, list[list[int]], list[Tensor]]]</code> <p>A list of tuples, where each tuple contains the filtered boxes, paths, and scores for an item in the batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; boxes_batch = [torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]]), torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]])]\n&gt;&gt;&gt; paths_batch = [[[4], [4, 6], [4, 5], [], []], [[4], [4, 6], [4, 5], [], []]]\n&gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])], [torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])]]\n&gt;&gt;&gt; result = batch_filter_empty_paths(boxes_batch, paths_batch, scores_batch)\n&gt;&gt;&gt; len(result)\n2\n&gt;&gt;&gt; result[0][0] # boxes for first batch item\ntensor([[482.2700, 395.7700, 241.9800],\n        [  8.1100, 156.8700, 152.9100],\n        [610.4200, 429.3800, 307.7000],\n        [103.8600, 200.9300, 197.5700]])\n&gt;&gt;&gt; result[0][1] # paths for first batch item\n[[4], [4, 6], [4, 5]]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def batch_filter_empty_paths(predicted_boxes: list[torch.Tensor], predicted_paths: list[list[list[int]]], predicted_path_scores: list[list[torch.Tensor]]) -&gt; list[tuple[torch.Tensor, list[list[int]], list[torch.Tensor]]]:\n    \"\"\"Applies empty path filtering to a batch of predictions.\n\n    This function maps the `filter_empty_paths` function over a batch of\n    predicted boxes, paths, and scores.\n\n    Parameters\n    ----------\n    predicted_boxes : list[torch.Tensor]\n        A batch of bounding box tensors.\n    predicted_paths : list[list[list[int]]]\n        A batch of predicted path lists.\n    predicted_path_scores : list[list[torch.Tensor]]\n        A batch of predicted path score lists.\n\n    Returns\n    -------\n    list[tuple[torch.Tensor, list[list[int]], list[torch.Tensor]]]\n        A list of tuples, where each tuple contains the filtered boxes,\n        paths, and scores for an item in the batch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; boxes_batch = [torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]]), torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]])]\n    &gt;&gt;&gt; paths_batch = [[[4], [4, 6], [4, 5], [], []], [[4], [4, 6], [4, 5], [], []]]\n    &gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])], [torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])]]\n    &gt;&gt;&gt; result = batch_filter_empty_paths(boxes_batch, paths_batch, scores_batch)\n    &gt;&gt;&gt; len(result)\n    2\n    &gt;&gt;&gt; result[0][0] # boxes for first batch item\n    tensor([[482.2700, 395.7700, 241.9800],\n            [  8.1100, 156.8700, 152.9100],\n            [610.4200, 429.3800, 307.7000],\n            [103.8600, 200.9300, 197.5700]])\n    &gt;&gt;&gt; result[0][1] # paths for first batch item\n    [[4], [4, 6], [4, 5]]\n    \"\"\"\n    B = len(predicted_paths)\n    return list(itertools.starmap(filter_empty_paths, zip(predicted_boxes, predicted_paths, predicted_path_scores)))\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.batch_truncate_paths_conditionals","title":"<code>batch_truncate_paths_conditionals(predicted_paths, predicted_path_scores, threshold=0.25)</code>","text":"<p>Applies conditional probability truncation to a batch of path lists.</p> <p>This function maps the <code>truncate_paths_conditionals</code> function over a batch of predicted paths and scores.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_paths</code> <code>list[list[list[int]]]</code> <p>A batch of path lists. Each item in the outer list corresponds to an item in the batch.</p> required <code>predicted_path_scores</code> <code>list[list[Tensor]]</code> <p>A batch of score lists, corresponding to <code>predicted_paths</code>.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold to use for truncation, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>list[tuple[list[list[int]], list[Tensor]]]</code> <p>A list of tuples, where each tuple contains the truncated paths and scores for an item in the batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; paths_batch = [[[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]], [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]]\n&gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])], [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]]\n&gt;&gt;&gt; batch_truncate_paths_conditionals(paths_batch, scores_batch, 0.589)\n[([[4, 2], [4, 6], [4, 5], [], []], [tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]), ([[4, 2], [4, 6], [4, 5], [], []], [tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])])]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def batch_truncate_paths_conditionals(predicted_paths: list[list[list[int]]], predicted_path_scores: list[list[torch.Tensor]], threshold: float = 0.25) -&gt; list[tuple[list[list[int]], list[torch.Tensor]]]:\n    \"\"\"Applies conditional probability truncation to a batch of path lists.\n\n    This function maps the `truncate_paths_conditionals` function over a\n    batch of predicted paths and scores.\n\n    Parameters\n    ----------\n    predicted_paths : list[list[list[int]]]\n        A batch of path lists. Each item in the outer list corresponds to\n        an item in the batch.\n    predicted_path_scores : list[list[torch.Tensor]]\n        A batch of score lists, corresponding to `predicted_paths`.\n    threshold : float, optional\n        The probability threshold to use for truncation, by default 0.25.\n\n    Returns\n    -------\n    list[tuple[list[list[int]], list[torch.Tensor]]]\n        A list of tuples, where each tuple contains the truncated paths and\n        scores for an item in the batch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; paths_batch = [[[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]], [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]]\n    &gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])], [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]]\n    &gt;&gt;&gt; batch_truncate_paths_conditionals(paths_batch, scores_batch, 0.589)\n    [([[4, 2], [4, 6], [4, 5], [], []], [tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]), ([[4, 2], [4, 6], [4, 5], [], []], [tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])])]\n    \"\"\"\n    B = len(predicted_paths)\n    return list(itertools.starmap(truncate_paths_conditionals, zip(predicted_paths, predicted_path_scores, itertools.repeat(threshold, B))))\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.batch_truncate_paths_marginals","title":"<code>batch_truncate_paths_marginals(predicted_paths, predicted_path_scores, threshold=0.25)</code>","text":"<p>Applies marginal probability truncation to a batch of path lists.</p> <p>This function maps the <code>truncate_paths_marginals</code> function over a batch of predicted paths and scores.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_paths</code> <code>list[list[list[int]]]</code> <p>A batch of path lists. Each item in the outer list corresponds to an item in the batch.</p> required <code>predicted_path_scores</code> <code>list[list[Tensor]]</code> <p>A batch of score lists, corresponding to <code>predicted_paths</code>.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold to use for truncation, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>list[tuple[list[list[int]], list[Tensor]]]</code> <p>A list of tuples, where each tuple contains the truncated paths and scores for an item in the batch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; paths_batch = [[[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]], [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]]\n&gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])], [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]]\n&gt;&gt;&gt; batch_truncate_paths_marginals(paths_batch, scores_batch, 0.589)\n[([[4], [4, 6], [4, 5], [], []], [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]), ([[4], [4, 6], [4, 5], [], []], [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])])]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def batch_truncate_paths_marginals(predicted_paths: list[list[list[int]]], predicted_path_scores: list[list[torch.Tensor]], threshold: float = 0.25) -&gt; list[tuple[list[list[int]], list[torch.Tensor]]]:\n    \"\"\"Applies marginal probability truncation to a batch of path lists.\n\n    This function maps the `truncate_paths_marginals` function over a\n    batch of predicted paths and scores.\n\n    Parameters\n    ----------\n    predicted_paths : list[list[list[int]]]\n        A batch of path lists. Each item in the outer list corresponds to\n        an item in the batch.\n    predicted_path_scores : list[list[torch.Tensor]]\n        A batch of score lists, corresponding to `predicted_paths`.\n    threshold : float, optional\n        The probability threshold to use for truncation, by default 0.25.\n\n    Returns\n    -------\n    list[tuple[list[list[int]], list[torch.Tensor]]]\n        A list of tuples, where each tuple contains the truncated paths and\n        scores for an item in the batch.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; paths_batch = [[[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]], [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]]\n    &gt;&gt;&gt; scores_batch = [[torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])], [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]]\n    &gt;&gt;&gt; batch_truncate_paths_marginals(paths_batch, scores_batch, 0.589)\n    [([[4], [4, 6], [4, 5], [], []], [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]), ([[4], [4, 6], [4, 5], [], []], [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])])]\n    \"\"\"\n    B = len(predicted_paths)\n    return list(itertools.starmap(truncate_paths_marginals, zip(predicted_paths, predicted_path_scores, itertools.repeat(threshold, B))))\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.construct_parent_childset_tree","title":"<code>construct_parent_childset_tree(tree)</code>","text":"<p>Converts a {child: parent} tree into a {parent: set[children]} tree.</p> <p>This function inverts the standard {child: parent} structure, creating a dictionary for navigating the tree top-down.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the tree in a top-down format, e.g., <code>{parent: set[children]}</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; childparent_tree = {0:1, 1:2, 3:2, 4:5}\n&gt;&gt;&gt; construct_parent_childset_tree(childparent_tree)\n{1: {0}, 2: {1, 3}, 5: {4}}\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def construct_parent_childset_tree(tree: dict[Hashable, Hashable]) -&gt; dict[Hashable, set]:\n    \"\"\"Converts a {child: parent} tree into a {parent: set[children]} tree.\n\n    This function inverts the standard {child: parent} structure, creating\n    a dictionary for navigating the tree top-down.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree in a top-down format,\n        e.g., `{parent: set[children]}`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; childparent_tree = {0:1, 1:2, 3:2, 4:5}\n    &gt;&gt;&gt; construct_parent_childset_tree(childparent_tree)\n    {1: {0}, 2: {1, 3}, 5: {4}}\n    \"\"\"\n    parent_childset_tree = {}\n    for child, parent in tree.items():\n        if parent not in parent_childset_tree:\n            parent_childset_tree[parent] = set()\n        parent_childset_tree[parent].add(child)\n    return parent_childset_tree\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.construct_parent_childtensor_tree","title":"<code>construct_parent_childtensor_tree(tree, device=None)</code>","text":"<p>Converts a {child: parent} tree into a {parent: tensor[children]} tree.</p> <p>This function inverts the standard {child: parent} structure, creating a dictionary for navigating the tree top-down.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the tree in a top-down format, e.g., <code>{parent: tensor[children]}</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; childparent_tree = {0:1, 1:2, 3:2, 4:5}\n&gt;&gt;&gt; construct_parent_childtensor_tree(childparent_tree)\n{1: tensor([0]), 2: tensor([1, 3]), 5: tensor([4])}\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def construct_parent_childtensor_tree(tree: dict[Hashable, Hashable], device=None) -&gt; dict[Hashable, torch.Tensor]:\n    \"\"\"Converts a {child: parent} tree into a {parent: tensor[children]} tree.\n\n    This function inverts the standard {child: parent} structure, creating\n    a dictionary for navigating the tree top-down.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree in a top-down format,\n        e.g., `{parent: tensor[children]}`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; childparent_tree = {0:1, 1:2, 3:2, 4:5}\n    &gt;&gt;&gt; construct_parent_childtensor_tree(childparent_tree)\n    {1: tensor([0]), 2: tensor([1, 3]), 5: tensor([4])}\n    \"\"\"\n    childset_tree = construct_parent_childset_tree(tree)\n    return {k: torch.tensor(list(v), device=device) for k,v  in childset_tree.items()} \n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.filter_empty_paths","title":"<code>filter_empty_paths(predicted_boxes, predicted_paths, predicted_path_scores)</code>","text":"<p>Filters out predictions with empty paths.</p> <p>After truncation, some paths may become empty. This function removes those empty paths along with their corresponding scores and bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_boxes</code> <code>Tensor</code> <p>A 2D tensor of bounding box predictions, where columns correspond to individual predictions (e.g., shape [4, N]).</p> required <code>predicted_paths</code> <code>list[list[int]]</code> <p>A list of predicted paths.</p> required <code>predicted_path_scores</code> <code>list[Tensor]</code> <p>A list of predicted path scores.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, list[list[int]], list[Tensor]]</code> <p>A tuple containing the filtered boxes, paths, and scores, with empty path predictions removed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; boxes = torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]])\n&gt;&gt;&gt; paths = [[4], [4, 6], [4, 5], [], []]\n&gt;&gt;&gt; scores = [torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])]\n&gt;&gt;&gt; f_boxes, f_paths, f_scores = filter_empty_paths(boxes, paths, scores)\n&gt;&gt;&gt; f_boxes\ntensor([[482.2700, 395.7700, 241.9800],\n        [  8.1100, 156.8700, 152.9100],\n        [610.4200, 429.3800, 307.7000],\n        [103.8600, 200.9300, 197.5700]])\n&gt;&gt;&gt; f_paths\n[[4], [4, 6], [4, 5]]\n&gt;&gt;&gt; f_scores\n[tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765])]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def filter_empty_paths(predicted_boxes: torch.Tensor, predicted_paths: list[list[int]], predicted_path_scores: list[torch.Tensor]) -&gt; tuple[torch.Tensor, list[list[int]], list[torch.Tensor]]:\n    \"\"\"Filters out predictions with empty paths.\n\n    After truncation, some paths may become empty. This function removes\n    those empty paths along with their corresponding scores and bounding\n    boxes.\n\n    Parameters\n    ----------\n    predicted_boxes : torch.Tensor\n        A 2D tensor of bounding box predictions, where columns correspond\n        to individual predictions (e.g., shape [4, N]).\n    predicted_paths : list[list[int]]\n        A list of predicted paths.\n    predicted_path_scores : list[torch.Tensor]\n        A list of predicted path scores.\n\n    Returns\n    -------\n    tuple[torch.Tensor, list[list[int]], list[torch.Tensor]]\n        A tuple containing the filtered boxes, paths, and scores,\n        with empty path predictions removed.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; boxes = torch.tensor([[482.27, 395.77, 241.98, 359.60, 258.38], [8.11, 156.87, 152.91, 335.40, 24.81], [610.42, 429.38, 307.70, 382.68, 413.79], [103.86, 200.93, 197.57, 352.40, 197.61]])\n    &gt;&gt;&gt; paths = [[4], [4, 6], [4, 5], [], []]\n    &gt;&gt;&gt; scores = [torch.tensor([0.9896]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([]), torch.tensor([])]\n    &gt;&gt;&gt; f_boxes, f_paths, f_scores = filter_empty_paths(boxes, paths, scores)\n    &gt;&gt;&gt; f_boxes\n    tensor([[482.2700, 395.7700, 241.9800],\n            [  8.1100, 156.8700, 152.9100],\n            [610.4200, 429.3800, 307.7000],\n            [103.8600, 200.9300, 197.5700]])\n    &gt;&gt;&gt; f_paths\n    [[4], [4, 6], [4, 5]]\n    &gt;&gt;&gt; f_scores\n    [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765])]\n    \"\"\"\n    keep_idx = [i for i, path in enumerate(predicted_paths) if len(path) &gt; 0]\n    return (\n        predicted_boxes[:,keep_idx],\n        [predicted_paths[k] for k in keep_idx],\n        [predicted_path_scores[k] for k in keep_idx]\n    )\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.find_closest_permitted_parent","title":"<code>find_closest_permitted_parent(node, tree, permitted_nodes)</code>","text":"<p>Finds the first ancestor of a node that is in a permitted set.</p> <p>This function walks up the ancestral chain of a node (using the {child: parent} tree) and returns the first ancestor it finds that is present in the <code>permitted_nodes</code> set.</p> <p>If no ancestor (including the node itself) is in the set, or if the node is not in the tree to begin with, it returns None.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Hashable</code> <p>The ID of the node to start searching from.</p> required <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs that are considered \"permitted\".</p> required <p>Returns:</p> Type Description <code>Hashable | None</code> <p>The ID of the closest permitted ancestor, or None if none is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n&gt;&gt;&gt; permitted = {0, 2, 5}\n&gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n&gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n&gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def find_closest_permitted_parent(\n    node: Hashable,\n    tree: dict[Hashable, Hashable],\n    permitted_nodes: set[Hashable],\n) -&gt; Hashable | None:\n    \"\"\"Finds the first ancestor of a node that is in a permitted set.\n\n    This function walks up the ancestral chain of a node (using the\n    {child: parent} tree) and returns the first ancestor it finds\n    that is present in the `permitted_nodes` set.\n\n    If no ancestor (including the node itself) is in the set,\n    or if the node is not in the tree to begin with, it returns None.\n\n    Parameters\n    ----------\n    node : Hashable\n        The ID of the node to start searching from.\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs that are considered \"permitted\".\n\n    Returns\n    -------\n    Hashable | None\n        The ID of the closest permitted ancestor, or None if none is found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n    &gt;&gt;&gt; permitted = {0, 2, 5}\n    &gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n    &gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n    &gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n    \"\"\"\n    if node not in tree:\n        return None\n    parent = tree[node]\n    while parent not in permitted_nodes:\n        if parent in tree:\n            parent = tree[parent]\n        else:\n            return None\n    return parent\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.get_ancestor_chain_lens","title":"<code>get_ancestor_chain_lens(tree)</code>","text":"<p>Get lengths of ancestor chains in a { child: parent } dictionary tree</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n{3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in { child: parent } format.</p> required <p>Returns:</p> Name Type Description <code>lengths</code> <code>dict[Hashable, int]</code> <p>The lengths of the path to the root from each node { node: length }</p> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_ancestor_chain_lens(tree: dict[Hashable, Hashable]) -&gt; dict[Hashable, int]:\n    '''\n    Get lengths of ancestor chains in a { child: parent } dictionary tree\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n    {3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n\n    Parameters\n    ----------\n    tree: dict[Hashable, Hashable]\n        A tree in { child: parent } format.\n\n    Returns\n    -------\n    lengths: dict[Hashable, int]\n        The lengths of the path to the root from each node { node: length }\n\n    '''\n    return preorder_apply(tree, _increment_chain_len)\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.get_roots","title":"<code>get_roots(tree)</code>","text":"<p>Finds all root nodes in a {child: parent} tree.</p> <p>A root node is defined as any node that is not a child of another node in the tree (i.e., its ancestor chain length is 1).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>A list of all root nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n&gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n[2, 6]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_roots(tree: dict[Hashable, Hashable]) -&gt; list[Hashable]:\n    \"\"\"Finds all root nodes in a {child: parent} tree.\n\n    A root node is defined as any node that is not a child of another\n    node in the tree (i.e., its ancestor chain length is 1).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    list[Hashable]\n        A list of all root nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n    &gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n    [2, 6]\n    \"\"\"\n    ancestor_chain_lens = get_ancestor_chain_lens(tree)\n    return [node for node in ancestor_chain_lens if ancestor_chain_lens[node] == 1]\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.invert_childparent_tree","title":"<code>invert_childparent_tree(tree)</code>","text":"<p>Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.</p> <p>This function inverts the standard {child: parent} structure, creating a nested dictionary that starts from the root(s). It uses <code>preorder_apply</code> to traverse the tree top-down and build the nested structure.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the tree in a top-down format, e.g., <code>{root: {child: {grandchild: {}}}}</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n&gt;&gt;&gt; invert_childparent_tree(tree)\n{2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def invert_childparent_tree(tree: dict[Hashable, Hashable]) -&gt; dict:\n    \"\"\"Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.\n\n    This function inverts the standard {child: parent} structure, creating\n    a nested dictionary that starts from the root(s). It uses\n    `preorder_apply` to traverse the tree top-down and build the\n    nested structure.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree in a top-down format,\n        e.g., `{root: {child: {grandchild: {}}}}`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n    &gt;&gt;&gt; invert_childparent_tree(tree)\n    {2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n    \"\"\"\n    parentchild_tree = {}\n    preorder_apply(tree, _append_to_parentchild_tree, parentchild_tree)\n    return parentchild_tree\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.optimal_hierarchical_path","title":"<code>optimal_hierarchical_path(class_scores, inverted_tree, roots)</code>","text":"<p>Finds optimal paths and extracts their corresponding scores.</p> <p>This function wraps <code>get_optimal_ancestral_chain</code> to find the single best greedy path for each detection, and then gathers the raw scores associated with each node in those paths.</p> <p>Parameters:</p> Name Type Description Default <code>class_scores</code> <code>list[Tensor]</code> <p>A list of confidence tensors, one per batch item. Each tensor should have shape (C, N), where C is the number of classes and N is the number of detections.</p> required <code>inverted_tree</code> <code>dict[int, Tensor]</code> <p>The class hierarchy in <code>{parent_id: tensor([child1, child2, ...])}</code> format.</p> required <code>roots</code> <code>Tensor</code> <p>A 1D tensor containing the integer IDs of the root nodes.</p> required <p>Returns:</p> Type Description <code>tuple[list[list[list[int]]], list[list[Tensor]]]</code> <p>A tuple containing two items: 1. <code>optimal_paths</code>: The nested list of paths, as returned    by <code>get_optimal_ancestral_chain</code>. 2. <code>optimal_path_scores</code>: A nested list of the same structure,    but containing 1D tensors of the scores for each path.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hierarchy = {1: 0, 2: 0, 3: 1, 4: 1, 5: 2, 6: 2}\n&gt;&gt;&gt; # C=7 classes, N=2 detections, B=1 batch item\n&gt;&gt;&gt; # Scores are shaped (C, N)\n&gt;&gt;&gt; scores = torch.tensor([\n...     [10., 10.],  # 0 (Root)\n...     [ 5.,  1.],  # 1 (Child of 0)\n...     [ 1.,  5.],  # 2 (Child of 0)\n...     [ 2.,  0.],  # 3 (Child of 1)\n...     [ 8.,  0.],  # 4 (Child of 1)\n...     [ 0.,  8.],  # 5 (Child of 2)\n...     [ 0.,  2.]   # 6 (Child of 2)\n... ], dtype=torch.float32)\n&gt;&gt;&gt; class_scores = [scores]\n&gt;&gt;&gt; inverted_tree = construct_parent_childtensor_tree(hierarchy, device=class_scores[0].device)\n&gt;&gt;&gt; roots = torch.tensor(get_roots(hierarchy), device=class_scores[0].device)\n&gt;&gt;&gt; paths, path_scores = optimal_hierarchical_path(class_scores, inverted_tree, roots)\n&gt;&gt;&gt; paths\n[[[0, 1, 4], [0, 2, 5]]]\n&gt;&gt;&gt; path_scores\n[[tensor([10.,  5.,  8.]), tensor([10.,  5.,  8.])]]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def optimal_hierarchical_path(class_scores: list[torch.Tensor], inverted_tree: dict[int, torch.Tensor], roots: torch.Tensor) -&gt; tuple[list[list[list[int]]], list[list[torch.Tensor]]]:\n    \"\"\"Finds optimal paths and extracts their corresponding scores.\n\n    This function wraps `get_optimal_ancestral_chain` to find the\n    single best greedy path for each detection, and then gathers\n    the raw scores associated with each node in those paths.\n\n    Parameters\n    ----------\n    class_scores : list[torch.Tensor]\n        A list of confidence tensors, one per batch item. Each tensor\n        should have shape (C, N), where C is the number of classes\n        and N is the number of detections.\n    inverted_tree : dict[int, torch.Tensor]\n        The class hierarchy in `{parent_id: tensor([child1, child2, ...])}`\n        format.\n    roots : torch.Tensor\n        A 1D tensor containing the integer IDs of the root nodes.\n\n    Returns\n    -------\n    tuple[list[list[list[int]]], list[list[torch.Tensor]]]\n        A tuple containing two items:\n        1. `optimal_paths`: The nested list of paths, as returned\n           by `get_optimal_ancestral_chain`.\n        2. `optimal_path_scores`: A nested list of the same structure,\n           but containing 1D tensors of the scores for each path.\n\n    Examples\n    --------\n    &gt;&gt;&gt; hierarchy = {1: 0, 2: 0, 3: 1, 4: 1, 5: 2, 6: 2}\n    &gt;&gt;&gt; # C=7 classes, N=2 detections, B=1 batch item\n    &gt;&gt;&gt; # Scores are shaped (C, N)\n    &gt;&gt;&gt; scores = torch.tensor([\n    ...     [10., 10.],  # 0 (Root)\n    ...     [ 5.,  1.],  # 1 (Child of 0)\n    ...     [ 1.,  5.],  # 2 (Child of 0)\n    ...     [ 2.,  0.],  # 3 (Child of 1)\n    ...     [ 8.,  0.],  # 4 (Child of 1)\n    ...     [ 0.,  8.],  # 5 (Child of 2)\n    ...     [ 0.,  2.]   # 6 (Child of 2)\n    ... ], dtype=torch.float32)\n    &gt;&gt;&gt; class_scores = [scores]\n    &gt;&gt;&gt; inverted_tree = construct_parent_childtensor_tree(hierarchy, device=class_scores[0].device)\n    &gt;&gt;&gt; roots = torch.tensor(get_roots(hierarchy), device=class_scores[0].device)\n    &gt;&gt;&gt; paths, path_scores = optimal_hierarchical_path(class_scores, inverted_tree, roots)\n    &gt;&gt;&gt; paths\n    [[[0, 1, 4], [0, 2, 5]]]\n    &gt;&gt;&gt; path_scores\n    [[tensor([10.,  5.,  8.]), tensor([10.,  5.,  8.])]]\n    \"\"\"\n    bpaths = []\n    bscores = []\n    for b, confidence in enumerate(class_scores):\n        paths = []\n        scores = []\n        for i in range(confidence.shape[1]):\n            confidence_row = confidence[..., i]\n            path = []\n            path_score = []\n            siblings = roots\n            while siblings is not None:\n                best = confidence_row.index_select(0, siblings).argmax()\n                best_node_id = int(siblings[best])\n                path.append(best_node_id)\n                path_score.append(confidence_row[best_node_id])\n                siblings = inverted_tree[best_node_id] if best_node_id in inverted_tree else None\n            paths.append(path)\n            scores.append(torch.stack(path_score))\n        bpaths.append(paths)\n        bscores.append(scores)\n    return bpaths, bscores\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.optimal_hierarchical_paths","title":"<code>optimal_hierarchical_paths(class_scores, hierarchy)</code>","text":"<p>.. deprecated:: 0.X.X    This function is deprecated as it re-computes the hierarchy    on every call, causing a performance bottleneck.    Use a <code>Hierarchy</code> object to pre-compute the <code>inverted_tree</code>    and <code>roots</code>, and then call <code>optimal_hierarchical_path</code> directly.</p> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def optimal_hierarchical_paths(class_scores: list[torch.Tensor], hierarchy: dict[int, int]) -&gt; tuple[list[list[list[int]]], list[list[torch.Tensor]]]:\n    \"\"\"\n    .. deprecated:: 0.X.X\n       This function is deprecated as it re-computes the hierarchy\n       on every call, causing a performance bottleneck.\n       Use a `Hierarchy` object to pre-compute the `inverted_tree`\n       and `roots`, and then call `optimal_hierarchical_path` directly.\n    \"\"\"\n    inverted_tree = construct_parent_childtensor_tree(hierarchy, device=class_scores[0].device)\n    roots = torch.tensor(get_roots(hierarchy), device=class_scores[0].device)\n    return optimal_hierarchical_path(class_scores, inverted_tree, roots)\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.preorder_apply","title":"<code>preorder_apply(tree, f, *args)</code>","text":"<p>Applies a function to all nodes in a tree in a pre-order (top-down) fashion.</p> <p>This function works by first finding an ancestor path (from leaf to root). It then applies the function <code>f</code> to the root (or highest unvisited node) and iterates down the path, applying <code>f</code> to each child and passing in the result from its parent. This top-down application is a pre-order traversal.</p> <p>It uses memoization (the <code>visited</code> dict) to ensure that <code>f</code> is applied to each node only once, even in multi-branch trees.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>f</code> <code>Callable</code> <p>The function to apply to each node. Its signature must be <code>f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any</code>.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments to be passed to every call of <code>f</code>.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[Hashable, Any]</code> <p>A dictionary mapping each node ID to the result of <code>f(node, ...)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n&gt;&gt;&gt; def f(node, parent_depth):\n...     # parent_depth is the result from the parent node\n...     return 1 if parent_depth is None else parent_depth + 1\n...\n&gt;&gt;&gt; preorder_apply(tree, f)\n{2: 1, 1: 2, 0: 3, 3: 2}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def preorder_apply(tree: dict[Hashable, Hashable], f: Callable, *args: Any) -&gt; dict[Hashable, Any]:\n    \"\"\"Applies a function to all nodes in a tree in a pre-order (top-down) fashion.\n\n    This function works by first finding an ancestor path (from leaf to root).\n    It then applies the function `f` to the root (or highest unvisited node)\n    and iterates *down* the path, applying `f` to each child and passing in\n    the result from its parent. This top-down application is a pre-order\n    traversal.\n\n    It uses memoization (the `visited` dict) to ensure that `f` is\n    applied to each node only once, even in multi-branch trees.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    f : Callable\n        The function to apply to each node. Its signature must be\n        `f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any`.\n    *args: Any\n        Additional positional arguments to be passed to every call of `f`.\n\n    Returns\n    -------\n    dict[Hashable, Any]\n        A dictionary mapping each node ID to the result of `f(node, ...)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n    &gt;&gt;&gt; def f(node, parent_depth):\n    ...     # parent_depth is the result from the parent node\n    ...     return 1 if parent_depth is None else parent_depth + 1\n    ...\n    &gt;&gt;&gt; preorder_apply(tree, f)\n    {2: 1, 1: 2, 0: 3, 3: 2}\n    \"\"\"\n    visited = {}\n    for node in tree:\n        path = [node]\n        while (node in tree) and (node not in visited):\n            node = tree[node]\n            path.append(node)\n        if node not in visited:\n            visited[node] = f(node, None, *args)\n        for i in range(-2, -len(path) - 1, -1):\n            visited[path[i]] = f(path[i], visited[path[i+1]], *args)\n    return visited\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.tree_walk","title":"<code>tree_walk(tree, node)</code>","text":"<p>Walks up the ancestor chain from a starting node.</p> <p>This generator yields the starting node first, then its parent, its grandparent, and so on, until a root (a node not present as a key in the tree) is reached.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>node</code> <code>Hashable</code> <p>The node to start the walk from.</p> required <p>Yields:</p> Type Description <code>Iterator[Hashable]</code> <p>An iterator of node IDs in the ancestor chain, starting with the given node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n&gt;&gt;&gt; list(tree_walk(tree, 0))\n[0, 1, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 3))\n[3, 4, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 2))\n[2]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def tree_walk(tree: dict[Hashable, Hashable], node: Hashable) -&gt; Iterator[Hashable]:\n    \"\"\"Walks up the ancestor chain from a starting node.\n\n    This generator yields the starting node first, then its parent,\n    its grandparent, and so on, until a root (a node not\n    present as a key in the tree) is reached.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    node : Hashable\n        The node to start the walk from.\n\n    Yields\n    ------\n    Iterator[Hashable]\n        An iterator of node IDs in the ancestor chain, starting\n        with the given node.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n    &gt;&gt;&gt; list(tree_walk(tree, 0))\n    [0, 1, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 3))\n    [3, 4, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 2))\n    [2]\n    \"\"\"\n    yield node\n    while node in tree:\n        node = tree[node]\n        yield node\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.trim_childparent_tree","title":"<code>trim_childparent_tree(tree, permitted_nodes)</code>","text":"<p>Trims a {child: parent} tree to only include permitted nodes.</p> <p>This function first remaps every node in the tree to its closest permitted ancestor. It then filters this map, keeping only the entries where the node (the key) is also in the <code>permitted_nodes</code> set.</p> <p>The result is a new {child: parent} tree containing only permitted nodes, mapped to their closest permitted ancestor (which will be another permitted node or None).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs to keep.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Hashable | None]</code> <p>A new {child: parent} tree containing only permitted nodes, each re-mapped to its closest permitted ancestor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n&gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n&gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n{0: 2, 2: 5}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def trim_childparent_tree(\n    tree: dict[Hashable, Hashable], permitted_nodes: set[Hashable]\n) -&gt; dict[Hashable, Hashable | None]:\n    \"\"\"Trims a {child: parent} tree to only include permitted nodes.\n\n    This function first remaps every node in the tree to its closest\n    permitted ancestor. It then filters this map, keeping only the\n    entries where the node (the key) is *also* in the `permitted_nodes`\n    set.\n\n    The result is a new {child: parent} tree containing *only*\n    permitted nodes, mapped to their closest permitted ancestor\n    (which will be another permitted node or None).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs to keep.\n\n    Returns\n    -------\n    dict[Hashable, Hashable | None]\n        A new {child: parent} tree containing only permitted nodes,\n        each re-mapped to its closest permitted ancestor.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n    &gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n    &gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n    {0: 2, 2: 5}\n    \"\"\"\n    new_tree = {}\n    for node in tree:\n        closest_permitted_parent = find_closest_permitted_parent(node, tree, permitted_nodes)\n        new_tree[node] = closest_permitted_parent\n    for node in list(new_tree.keys()):\n        if new_tree[node] is None or (node not in permitted_nodes):\n            del new_tree[node]\n    return new_tree\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.truncate_path_conditionals","title":"<code>truncate_path_conditionals(path, score, threshold=0.25)</code>","text":"<p>Truncates a path based on a conditional probability threshold.</p> <p>This function iterates through a path and its corresponding conditional probabilities, stopping at the first element where the probability is below the given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>list[int]</code> <p>A list of category indices representing the path.</p> required <code>score</code> <code>Tensor</code> <p>A 1D tensor where each element is the conditional probability of the corresponding category in the path.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold below which to truncate, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>tuple[list[int], Tensor]</code> <p>A tuple containing the truncated path and its corresponding scores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; path = [4, 7]\n&gt;&gt;&gt; score = torch.tensor([0.5412, 0.4371])\n&gt;&gt;&gt; truncate_path_conditionals(path, score, threshold=0.589)\n([], tensor([]))\n&gt;&gt;&gt; path = [4, 2]\n&gt;&gt;&gt; score = torch.tensor([0.9896, 0.5891])\n&gt;&gt;&gt; truncate_path_conditionals(path, score, threshold=0.589)\n([4, 2], tensor([0.9896, 0.5891]))\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def truncate_path_conditionals(path: list[int], score: torch.Tensor, threshold: float = 0.25) -&gt; tuple[list[int], torch.Tensor]:\n    \"\"\"Truncates a path based on a conditional probability threshold.\n\n    This function iterates through a path and its corresponding conditional\n    probabilities, stopping at the first element where the probability\n    is below the given threshold.\n\n    Parameters\n    ----------\n    path : list[int]\n        A list of category indices representing the path.\n    score : torch.Tensor\n        A 1D tensor where each element is the conditional probability\n        of the corresponding category in the path.\n    threshold : float, optional\n        The probability threshold below which to truncate, by default 0.25.\n\n    Returns\n    -------\n    tuple[list[int], torch.Tensor]\n        A tuple containing the truncated path and its corresponding scores.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; path = [4, 7]\n    &gt;&gt;&gt; score = torch.tensor([0.5412, 0.4371])\n    &gt;&gt;&gt; truncate_path_conditionals(path, score, threshold=0.589)\n    ([], tensor([]))\n    &gt;&gt;&gt; path = [4, 2]\n    &gt;&gt;&gt; score = torch.tensor([0.9896, 0.5891])\n    &gt;&gt;&gt; truncate_path_conditionals(path, score, threshold=0.589)\n    ([4, 2], tensor([0.9896, 0.5891]))\n    \"\"\"\n    truncated_path, truncated_score = [], []\n    for category, p in zip(path, score):\n        if p &lt; threshold:\n            break\n        truncated_path.append(category)\n    return truncated_path, score[:len(truncated_path)]\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.truncate_path_marginals","title":"<code>truncate_path_marginals(path, score, threshold=0.25)</code>","text":"<p>Truncates a path based on a marginal probability threshold.</p> <p>This function iterates through a path, calculating the cumulative product (marginal probability) of the scores. It stops at the first element where this cumulative product falls below the given threshold.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>list[int]</code> <p>A list of category indices representing the path.</p> required <code>score</code> <code>Tensor</code> <p>A 1D tensor where each element is the conditional probability of the corresponding category in the path.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold below which to truncate, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>tuple[list[int], Tensor]</code> <p>A tuple containing the truncated path and its corresponding scores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; path = [4, 2]\n&gt;&gt;&gt; score = torch.tensor([0.9896, 0.5891])\n&gt;&gt;&gt; truncate_path_marginals(path, score, threshold=0.589)\n([4], tensor([0.9896]))\n&gt;&gt;&gt; path = [4, 6]\n&gt;&gt;&gt; score = torch.tensor([0.9246, 0.7684])\n&gt;&gt;&gt; truncate_path_marginals(path, score, threshold=0.589)\n([4, 6], tensor([0.9246, 0.7684]))\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def truncate_path_marginals(path: list[int], score: torch.Tensor, threshold: float = 0.25) -&gt; tuple[list[int], torch.Tensor]:\n    \"\"\"Truncates a path based on a marginal probability threshold.\n\n    This function iterates through a path, calculating the cumulative\n    product (marginal probability) of the scores. It stops at the first\n    element where this cumulative product falls below the given threshold.\n\n    Parameters\n    ----------\n    path : list[int]\n        A list of category indices representing the path.\n    score : torch.Tensor\n        A 1D tensor where each element is the conditional probability\n        of the corresponding category in the path.\n    threshold : float, optional\n        The probability threshold below which to truncate, by default 0.25.\n\n    Returns\n    -------\n    tuple[list[int], torch.Tensor]\n        A tuple containing the truncated path and its corresponding scores.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; path = [4, 2]\n    &gt;&gt;&gt; score = torch.tensor([0.9896, 0.5891])\n    &gt;&gt;&gt; truncate_path_marginals(path, score, threshold=0.589)\n    ([4], tensor([0.9896]))\n    &gt;&gt;&gt; path = [4, 6]\n    &gt;&gt;&gt; score = torch.tensor([0.9246, 0.7684])\n    &gt;&gt;&gt; truncate_path_marginals(path, score, threshold=0.589)\n    ([4, 6], tensor([0.9246, 0.7684]))\n    \"\"\"\n    truncated_path, truncated_score = [], []\n    marginal_p = 1\n    for category, p in zip(path, score):\n        marginal_p *= p\n        if marginal_p &lt; threshold:\n            break\n        truncated_path.append(category)\n    return truncated_path, score[:len(truncated_path)]\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.truncate_paths_conditionals","title":"<code>truncate_paths_conditionals(predicted_paths, predicted_path_scores, threshold=0.25)</code>","text":"<p>Applies conditional probability truncation to a list of paths.</p> <p>This function iterates through lists of paths and scores, applying the <code>truncate_path_conditionals</code> function to each path-score pair.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_paths</code> <code>list[list[int]]</code> <p>A list of paths, where each path is a list of category indices.</p> required <code>predicted_path_scores</code> <code>list[Tensor]</code> <p>A list of 1D tensors, each corresponding to a path in <code>predicted_paths</code>.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold to pass to the truncation function, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>tuple[list[list[int]], list[Tensor]]</code> <p>A tuple containing the list of truncated paths and the list of their corresponding truncated scores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; paths = [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]\n&gt;&gt;&gt; scores = [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]\n&gt;&gt;&gt; tpaths, tscores = truncate_paths_conditionals(paths, scores, threshold=0.589)\n&gt;&gt;&gt; tpaths\n[[4, 2], [4, 6], [4, 5], [], []]\n&gt;&gt;&gt; tscores\n[tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def truncate_paths_conditionals(predicted_paths: list[list[int]], predicted_path_scores: list[torch.Tensor], threshold: float = 0.25) -&gt; tuple[list[list[int]], list[torch.Tensor]]:\n    \"\"\"Applies conditional probability truncation to a list of paths.\n\n    This function iterates through lists of paths and scores, applying\n    the `truncate_path_conditionals` function to each path-score pair.\n\n    Parameters\n    ----------\n    predicted_paths : list[list[int]]\n        A list of paths, where each path is a list of category indices.\n    predicted_path_scores : list[torch.Tensor]\n        A list of 1D tensors, each corresponding to a path in `predicted_paths`.\n    threshold : float, optional\n        The probability threshold to pass to the truncation function,\n        by default 0.25.\n\n    Returns\n    -------\n    tuple[list[list[int]], list[torch.Tensor]]\n        A tuple containing the list of truncated paths and the list of\n        their corresponding truncated scores.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; paths = [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]\n    &gt;&gt;&gt; scores = [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]\n    &gt;&gt;&gt; tpaths, tscores = truncate_paths_conditionals(paths, scores, threshold=0.589)\n    &gt;&gt;&gt; tpaths\n    [[4, 2], [4, 6], [4, 5], [], []]\n    &gt;&gt;&gt; tscores\n    [tensor([0.9896, 0.5891]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]\n    \"\"\"\n    tpaths, tscores = [], []\n    for paths, scores in zip(predicted_paths, predicted_path_scores):\n        tpath, tscore = truncate_path_conditionals(paths, scores, threshold=threshold)\n        tpaths.append(tpath), tscores.append(tscore)\n    return tpaths, tscores\n</code></pre>"},{"location":"api/path_utils/#hierarchical_loss.path_utils.truncate_paths_marginals","title":"<code>truncate_paths_marginals(predicted_paths, predicted_path_scores, threshold=0.25)</code>","text":"<p>Applies marginal probability truncation to a list of paths.</p> <p>This function iterates through lists of paths and scores, applying the <code>truncate_path_marginals</code> function to each path-score pair.</p> <p>Parameters:</p> Name Type Description Default <code>predicted_paths</code> <code>list[list[int]]</code> <p>A list of paths, where each path is a list of category indices.</p> required <code>predicted_path_scores</code> <code>list[Tensor]</code> <p>A list of 1D tensors, each corresponding to a path in <code>predicted_paths</code>.</p> required <code>threshold</code> <code>float</code> <p>The probability threshold to pass to the truncation function, by default 0.25.</p> <code>0.25</code> <p>Returns:</p> Type Description <code>tuple[list[list[int]], list[Tensor]]</code> <p>A tuple containing the list of truncated paths and the list of their corresponding truncated scores.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; paths = [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]\n&gt;&gt;&gt; scores = [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]\n&gt;&gt;&gt; tpaths, tscores = truncate_paths_marginals(paths, scores, threshold=0.589)\n&gt;&gt;&gt; tpaths\n[[4], [4, 6], [4, 5], [], []]\n&gt;&gt;&gt; tscores\n[tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]\n</code></pre> Source code in <code>hierarchical_loss/path_utils.py</code> <pre><code>def truncate_paths_marginals(predicted_paths: list[list[int]], predicted_path_scores: list[torch.Tensor], threshold: float = 0.25) -&gt; tuple[list[list[int]], list[torch.Tensor]]:\n    \"\"\"Applies marginal probability truncation to a list of paths.\n\n    This function iterates through lists of paths and scores, applying\n    the `truncate_path_marginals` function to each path-score pair.\n\n    Parameters\n    ----------\n    predicted_paths : list[list[int]]\n        A list of paths, where each path is a list of category indices.\n    predicted_path_scores : list[torch.Tensor]\n        A list of 1D tensors, each corresponding to a path in `predicted_paths`.\n    threshold : float, optional\n        The probability threshold to pass to the truncation function,\n        by default 0.25.\n\n    Returns\n    -------\n    tuple[list[list[int]], list[torch.Tensor]]\n        A tuple containing the list of truncated paths and the list of\n        their corresponding truncated scores.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; paths = [[4, 2], [4, 6], [4, 5], [4, 7], [4, 2]]\n    &gt;&gt;&gt; scores = [torch.tensor([0.9896, 0.5891]), torch.tensor([0.9246, 0.7684]), torch.tensor([0.8949, 0.8765]), torch.tensor([0.5412, 0.4371]), torch.tensor([0.5001, 0.0830])]\n    &gt;&gt;&gt; tpaths, tscores = truncate_paths_marginals(paths, scores, threshold=0.589)\n    &gt;&gt;&gt; tpaths\n    [[4], [4, 6], [4, 5], [], []]\n    &gt;&gt;&gt; tscores\n    [tensor([0.9896]), tensor([0.9246, 0.7684]), tensor([0.8949, 0.8765]), tensor([]), tensor([])]\n    \"\"\"\n    tpaths, tscores = [], []\n    for paths, scores in zip(predicted_paths, predicted_path_scores):\n        tpath, tscore = truncate_path_marginals(paths, scores, threshold=threshold)\n        tpaths.append(tpath), tscores.append(tscore)\n    return tpaths, tscores\n</code></pre>"},{"location":"api/tree_utils/","title":"Generic dict-based tree utilities","text":""},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.find_closest_permitted_parent","title":"<code>find_closest_permitted_parent(node, tree, permitted_nodes)</code>","text":"<p>Finds the first ancestor of a node that is in a permitted set.</p> <p>This function walks up the ancestral chain of a node (using the {child: parent} tree) and returns the first ancestor it finds that is present in the <code>permitted_nodes</code> set.</p> <p>If no ancestor (including the node itself) is in the set, or if the node is not in the tree to begin with, it returns None.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Hashable</code> <p>The ID of the node to start searching from.</p> required <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs that are considered \"permitted\".</p> required <p>Returns:</p> Type Description <code>Hashable | None</code> <p>The ID of the closest permitted ancestor, or None if none is found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n&gt;&gt;&gt; permitted = {0, 2, 5}\n&gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n&gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n&gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n2\n&gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n&gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def find_closest_permitted_parent(\n    node: Hashable,\n    tree: dict[Hashable, Hashable],\n    permitted_nodes: set[Hashable],\n) -&gt; Hashable | None:\n    \"\"\"Finds the first ancestor of a node that is in a permitted set.\n\n    This function walks up the ancestral chain of a node (using the\n    {child: parent} tree) and returns the first ancestor it finds\n    that is present in the `permitted_nodes` set.\n\n    If no ancestor (including the node itself) is in the set,\n    or if the node is not in the tree to begin with, it returns None.\n\n    Parameters\n    ----------\n    node : Hashable\n        The ID of the node to start searching from.\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs that are considered \"permitted\".\n\n    Returns\n    -------\n    Hashable | None\n        The ID of the closest permitted ancestor, or None if none is found.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {1: 2, 2: 3, 3: 4, 4: 5}\n    &gt;&gt;&gt; permitted = {0, 2, 5}\n    &gt;&gt;&gt; find_closest_permitted_parent(1, tree, permitted) # 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 is not in tree keys, returns None\n    &gt;&gt;&gt; tree[0] = 1 # Add 0 to the tree\n    &gt;&gt;&gt; find_closest_permitted_parent(0, tree, permitted) # 0 -&gt; 1 -&gt; 2 (permitted)\n    2\n    &gt;&gt;&gt; tree = {10: 20, 20: 30, 30: 40}\n    &gt;&gt;&gt; find_closest_permitted_parent(10, tree, {50, 60}) # No permitted ancestors, returns None\n    \"\"\"\n    if node not in tree:\n        return None\n    parent = tree[node]\n    while parent not in permitted_nodes:\n        if parent in tree:\n            parent = tree[parent]\n        else:\n            return None\n    return parent\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.get_ancestor_chain_lens","title":"<code>get_ancestor_chain_lens(tree)</code>","text":"<p>Get lengths of ancestor chains in a { child: parent } dictionary tree</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n{3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in { child: parent } format.</p> required <p>Returns:</p> Name Type Description <code>lengths</code> <code>dict[Hashable, int]</code> <p>The lengths of the path to the root from each node { node: length }</p> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_ancestor_chain_lens(tree: dict[Hashable, Hashable]) -&gt; dict[Hashable, int]:\n    '''\n    Get lengths of ancestor chains in a { child: parent } dictionary tree\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_ancestor_chain_lens({ 0:1, 1:2, 2:3, 4:5, 5:6, 7:8 })\n    {3: 1, 2: 2, 1: 3, 0: 4, 6: 1, 5: 2, 4: 3, 8: 1, 7: 2}\n\n    Parameters\n    ----------\n    tree: dict[Hashable, Hashable]\n        A tree in { child: parent } format.\n\n    Returns\n    -------\n    lengths: dict[Hashable, int]\n        The lengths of the path to the root from each node { node: length }\n\n    '''\n    return preorder_apply(tree, _increment_chain_len)\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.get_roots","title":"<code>get_roots(tree)</code>","text":"<p>Finds all root nodes in a {child: parent} tree.</p> <p>A root node is defined as any node that is not a child of another node in the tree (i.e., its ancestor chain length is 1).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>list[Hashable]</code> <p>A list of all root nodes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n&gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n[2, 6]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def get_roots(tree: dict[Hashable, Hashable]) -&gt; list[Hashable]:\n    \"\"\"Finds all root nodes in a {child: parent} tree.\n\n    A root node is defined as any node that is not a child of another\n    node in the tree (i.e., its ancestor chain length is 1).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    list[Hashable]\n        A list of all root nodes.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6}\n    &gt;&gt;&gt; get_roots(tree) # Roots are 2 and 6\n    [2, 6]\n    \"\"\"\n    ancestor_chain_lens = get_ancestor_chain_lens(tree)\n    return [node for node in ancestor_chain_lens if ancestor_chain_lens[node] == 1]\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.invert_childparent_tree","title":"<code>invert_childparent_tree(tree)</code>","text":"<p>Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.</p> <p>This function inverts the standard {child: parent} structure, creating a nested dictionary that starts from the root(s). It uses <code>preorder_apply</code> to traverse the tree top-down and build the nested structure.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the tree in a top-down format, e.g., <code>{root: {child: {grandchild: {}}}}</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n&gt;&gt;&gt; invert_childparent_tree(tree)\n{2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def invert_childparent_tree(tree: dict[Hashable, Hashable]) -&gt; dict:\n    \"\"\"Converts a {child: parent} tree into a nested {parent: {child: ...}} tree.\n\n    This function inverts the standard {child: parent} structure, creating\n    a nested dictionary that starts from the root(s). It uses\n    `preorder_apply` to traverse the tree top-down and build the\n    nested structure.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the tree in a top-down format,\n        e.g., `{root: {child: {grandchild: {}}}}`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2, 5: 6} # 0-&gt;1-&gt;2, 3-&gt;2, 5-&gt;6\n    &gt;&gt;&gt; invert_childparent_tree(tree)\n    {2: {1: {0: {}}, 3: {}}, 6: {5: {}}}\n    \"\"\"\n    parentchild_tree = {}\n    preorder_apply(tree, _append_to_parentchild_tree, parentchild_tree)\n    return parentchild_tree\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.preorder_apply","title":"<code>preorder_apply(tree, f, *args)</code>","text":"<p>Applies a function to all nodes in a tree in a pre-order (top-down) fashion.</p> <p>This function works by first finding an ancestor path (from leaf to root). It then applies the function <code>f</code> to the root (or highest unvisited node) and iterates down the path, applying <code>f</code> to each child and passing in the result from its parent. This top-down application is a pre-order traversal.</p> <p>It uses memoization (the <code>visited</code> dict) to ensure that <code>f</code> is applied to each node only once, even in multi-branch trees.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>f</code> <code>Callable</code> <p>The function to apply to each node. Its signature must be <code>f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any</code>.</p> required <code>*args</code> <code>Any</code> <p>Additional positional arguments to be passed to every call of <code>f</code>.</p> <code>()</code> <p>Returns:</p> Type Description <code>dict[Hashable, Any]</code> <p>A dictionary mapping each node ID to the result of <code>f(node, ...)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n&gt;&gt;&gt; def f(node, parent_depth):\n...     # parent_depth is the result from the parent node\n...     return 1 if parent_depth is None else parent_depth + 1\n...\n&gt;&gt;&gt; preorder_apply(tree, f)\n{2: 1, 1: 2, 0: 3, 3: 2}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def preorder_apply(tree: dict[Hashable, Hashable], f: Callable, *args: Any) -&gt; dict[Hashable, Any]:\n    \"\"\"Applies a function to all nodes in a tree in a pre-order (top-down) fashion.\n\n    This function works by first finding an ancestor path (from leaf to root).\n    It then applies the function `f` to the root (or highest unvisited node)\n    and iterates *down* the path, applying `f` to each child and passing in\n    the result from its parent. This top-down application is a pre-order\n    traversal.\n\n    It uses memoization (the `visited` dict) to ensure that `f` is\n    applied to each node only once, even in multi-branch trees.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    f : Callable\n        The function to apply to each node. Its signature must be\n        `f(node: Hashable, parent_result: Any, *args: Any) -&gt; Any`.\n    *args: Any\n        Additional positional arguments to be passed to every call of `f`.\n\n    Returns\n    -------\n    dict[Hashable, Any]\n        A dictionary mapping each node ID to the result of `f(node, ...)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; # Example: Calculate node depth (pre-order calculation)\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 2} # 0-&gt;1-&gt;2, 3-&gt;2\n    &gt;&gt;&gt; def f(node, parent_depth):\n    ...     # parent_depth is the result from the parent node\n    ...     return 1 if parent_depth is None else parent_depth + 1\n    ...\n    &gt;&gt;&gt; preorder_apply(tree, f)\n    {2: 1, 1: 2, 0: 3, 3: 2}\n    \"\"\"\n    visited = {}\n    for node in tree:\n        path = [node]\n        while (node in tree) and (node not in visited):\n            node = tree[node]\n            path.append(node)\n        if node not in visited:\n            visited[node] = f(node, None, *args)\n        for i in range(-2, -len(path) - 1, -1):\n            visited[path[i]] = f(path[i], visited[path[i+1]], *args)\n    return visited\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.tree_walk","title":"<code>tree_walk(tree, node)</code>","text":"<p>Walks up the ancestor chain from a starting node.</p> <p>This generator yields the starting node first, then its parent, its grandparent, and so on, until a root (a node not present as a key in the tree) is reached.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>The hierarchy tree, in {child: parent} format.</p> required <code>node</code> <code>Hashable</code> <p>The node to start the walk from.</p> required <p>Yields:</p> Type Description <code>Iterator[Hashable]</code> <p>An iterator of node IDs in the ancestor chain, starting with the given node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n&gt;&gt;&gt; list(tree_walk(tree, 0))\n[0, 1, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 3))\n[3, 4, 2]\n&gt;&gt;&gt; list(tree_walk(tree, 2))\n[2]\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def tree_walk(tree: dict[Hashable, Hashable], node: Hashable) -&gt; Iterator[Hashable]:\n    \"\"\"Walks up the ancestor chain from a starting node.\n\n    This generator yields the starting node first, then its parent,\n    its grandparent, and so on, until a root (a node not\n    present as a key in the tree) is reached.\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        The hierarchy tree, in {child: parent} format.\n    node : Hashable\n        The node to start the walk from.\n\n    Yields\n    ------\n    Iterator[Hashable]\n        An iterator of node IDs in the ancestor chain, starting\n        with the given node.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 3: 4, 4: 2}\n    &gt;&gt;&gt; list(tree_walk(tree, 0))\n    [0, 1, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 3))\n    [3, 4, 2]\n    &gt;&gt;&gt; list(tree_walk(tree, 2))\n    [2]\n    \"\"\"\n    yield node\n    while node in tree:\n        node = tree[node]\n        yield node\n</code></pre>"},{"location":"api/tree_utils/#hierarchical_loss.tree_utils.trim_childparent_tree","title":"<code>trim_childparent_tree(tree, permitted_nodes)</code>","text":"<p>Trims a {child: parent} tree to only include permitted nodes.</p> <p>This function first remaps every node in the tree to its closest permitted ancestor. It then filters this map, keeping only the entries where the node (the key) is also in the <code>permitted_nodes</code> set.</p> <p>The result is a new {child: parent} tree containing only permitted nodes, mapped to their closest permitted ancestor (which will be another permitted node or None).</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>dict[Hashable, Hashable]</code> <p>A tree in {child: parent} format.</p> required <code>permitted_nodes</code> <code>set[Hashable]</code> <p>A set of node IDs to keep.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Hashable | None]</code> <p>A new {child: parent} tree containing only permitted nodes, each re-mapped to its closest permitted ancestor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n&gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n&gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n{0: 2, 2: 5}\n</code></pre> Source code in <code>hierarchical_loss/tree_utils.py</code> <pre><code>def trim_childparent_tree(\n    tree: dict[Hashable, Hashable], permitted_nodes: set[Hashable]\n) -&gt; dict[Hashable, Hashable | None]:\n    \"\"\"Trims a {child: parent} tree to only include permitted nodes.\n\n    This function first remaps every node in the tree to its closest\n    permitted ancestor. It then filters this map, keeping only the\n    entries where the node (the key) is *also* in the `permitted_nodes`\n    set.\n\n    The result is a new {child: parent} tree containing *only*\n    permitted nodes, mapped to their closest permitted ancestor\n    (which will be another permitted node or None).\n\n    Parameters\n    ----------\n    tree : dict[Hashable, Hashable]\n        A tree in {child: parent} format.\n    permitted_nodes : set[Hashable]\n        A set of node IDs to keep.\n\n    Returns\n    -------\n    dict[Hashable, Hashable | None]\n        A new {child: parent} tree containing only permitted nodes,\n        each re-mapped to its closest permitted ancestor.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5} # 0-&gt;1-&gt;2-&gt;3-&gt;4-&gt;5\n    &gt;&gt;&gt; permitted = {0, 2, 5} # 0, 2, and 5 are permitted\n    &gt;&gt;&gt; trim_childparent_tree(tree, permitted)\n    {0: 2, 2: 5}\n    \"\"\"\n    new_tree = {}\n    for node in tree:\n        closest_permitted_parent = find_closest_permitted_parent(node, tree, permitted_nodes)\n        new_tree[node] = closest_permitted_parent\n    for node in list(new_tree.keys()):\n        if new_tree[node] is None or (node not in permitted_nodes):\n            del new_tree[node]\n    return new_tree\n</code></pre>"},{"location":"api/utils/","title":"Generic utilities","text":""},{"location":"api/utils/#hierarchical_loss.utils.argmax_from_subset","title":"<code>argmax_from_subset(scores, indices)</code>","text":"<p>Finds the argmax from a subset of indices.</p> <p>This function is type-agnostic and works for both NumPy arrays and PyTorch tensors.</p> <p>The core operation is performed on the last dimension of the tensor.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>ndarray | Tensor</code> <p>Tensor/array of scores with shape (*D, N).</p> required <code>indices</code> <code>ndarray | Tensor</code> <p>A 1D Tensor/array of viable indices with shape (K,).</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p>A tensor/array of shape (*D) containing the argmax index. The return type will match the type of <code>indices</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; scores_np = np.array([\n...     [10, 20, 30, 5, 40],\n...     [99, 88, 77, 66, 55]\n... ])\n&gt;&gt;&gt; indices_np = np.array([0, 2, 4])\n&gt;&gt;&gt; argmax_from_subset(scores_np, indices_np)\narray([4, 0])\n&gt;&gt;&gt;\n&gt;&gt;&gt; scores_pt = torch.tensor(scores_np)\n&gt;&gt;&gt; indices_pt = torch.tensor(indices_np)\n&gt;&gt;&gt; argmax_from_subset(scores_pt, indices_pt)\ntensor([4, 0])\n</code></pre> Source code in <code>hierarchical_loss/utils.py</code> <pre><code>def argmax_from_subset(scores: np.ndarray | torch.Tensor, indices: np.ndarray | torch.Tensor) -&gt; np.ndarray | torch.Tensor:\n    \"\"\"\n    Finds the argmax from a subset of indices.\n\n    This function is type-agnostic and works for both NumPy arrays\n    and PyTorch tensors.\n\n    The core operation is performed on the last dimension of the tensor.\n\n    Parameters\n    ----------\n    scores : np.ndarray | torch.Tensor\n        Tensor/array of scores with shape (*D, N).\n    indices : np.ndarray | torch.Tensor\n        A 1D Tensor/array of viable indices with shape (K,).\n\n    Returns\n    ----------\n    np.ndarray | torch.Tensor\n        A tensor/array of shape (*D) containing the argmax index.\n        The return type will match the type of `indices`.\n\n    Examples\n    ----------\n    &gt;&gt;&gt; scores_np = np.array([\n    ...     [10, 20, 30, 5, 40],\n    ...     [99, 88, 77, 66, 55]\n    ... ])\n    &gt;&gt;&gt; indices_np = np.array([0, 2, 4])\n    &gt;&gt;&gt; argmax_from_subset(scores_np, indices_np)\n    array([4, 0])\n    &gt;&gt;&gt;\n    &gt;&gt;&gt; scores_pt = torch.tensor(scores_np)\n    &gt;&gt;&gt; indices_pt = torch.tensor(indices_np)\n    &gt;&gt;&gt; argmax_from_subset(scores_pt, indices_pt)\n    tensor([4, 0])\n    \"\"\"\n    subset_scores = scores[..., indices]\n\n    local_argmax_indices = subset_scores.argmax(axis=-1)\n\n    return indices[local_argmax_indices]\n</code></pre>"},{"location":"api/utils/#hierarchical_loss.utils.dict_keyvalue_replace","title":"<code>dict_keyvalue_replace(old_dict, replacemap)</code>","text":"<p>Remaps both keys and values of a dictionary using a replacement map.</p> <p>Iterates through <code>old_dict</code>, using <code>replacemap</code> to find the new key and the new value. Assumes both keys and values from <code>old_dict</code> are valid keys in <code>replacemap</code>.</p> <p>Parameters:</p> Name Type Description Default <code>old_dict</code> <code>dict[Hashable, Hashable]</code> <p>The original dictionary. Both its keys and values must be hashable and exist as keys in <code>replacemap</code>.</p> required <code>replacemap</code> <code>dict[Hashable, Hashable]</code> <p>A dictionary mapping old keys/values to new hashable keys/values. Both its keys and values must be hashable.</p> required <p>Returns:</p> Type Description <code>dict[Hashable, Hashable]</code> <p>A new dictionary with remapped keys and values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; old_d = {'a': 'b', 'c': 'd'}\n&gt;&gt;&gt; r_map = {'a': 100, 'b': 200, 'c': 300, 'd': 400}\n&gt;&gt;&gt; dict_keyvalue_replace(old_d, r_map)\n{100: 200, 300: 400}\n</code></pre> Source code in <code>hierarchical_loss/utils.py</code> <pre><code>def dict_keyvalue_replace(old_dict: dict[Hashable, Hashable], replacemap: dict[Hashable, Hashable]) -&gt; dict[Hashable, Hashable]:\n    \"\"\"Remaps both keys and values of a dictionary using a replacement map.\n\n    Iterates through `old_dict`, using `replacemap` to find the new\n    key and the new value. Assumes both keys and values from\n    `old_dict` are valid keys in `replacemap`.\n\n    Parameters\n    ----------\n    old_dict : dict[Hashable, Hashable]\n        The original dictionary. Both its keys and values must\n        be hashable and exist as keys in `replacemap`.\n    replacemap : dict[Hashable, Hashable]\n        A dictionary mapping old keys/values to new hashable keys/values.\n        Both its keys and values must be hashable.\n\n    Returns\n    -------\n    dict[Hashable, Hashable]\n        A new dictionary with remapped keys and values.\n\n    Examples\n    --------\n    &gt;&gt;&gt; old_d = {'a': 'b', 'c': 'd'}\n    &gt;&gt;&gt; r_map = {'a': 100, 'b': 200, 'c': 300, 'd': 400}\n    &gt;&gt;&gt; dict_keyvalue_replace(old_d, r_map)\n    {100: 200, 300: 400}\n    \"\"\"\n    new_dict = {}\n    for key in old_dict:\n        new_dict[replacemap[key]] = replacemap[old_dict[key]]\n    return new_dict\n</code></pre>"},{"location":"api/utils/#hierarchical_loss.utils.log1mexp","title":"<code>log1mexp(x)</code>","text":"<p>Compute log(1 - exp(x)) in a numerically stable way.</p> <p>This function is designed to prevent the loss of precision that occurs when <code>x</code> is very close to zero (i.e., a small negative number). Directly computing <code>log(1 - exp(x))</code> can lead to catastrophic cancellation and result in <code>-inf</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor containing negative values (log-probabilities). The function is not designed for <code>x &gt;= 0</code>, as <code>1 - exp(x)</code> would be zero or negative, making the logarithm undefined.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed <code>log(1 - exp(x))</code> values, with the same shape as <code>x</code>.</p> Notes <p>The function uses two different mathematical identities based on the value of <code>x</code> to ensure stability:</p> <ol> <li>For <code>x &gt; -ln(2)</code> (i.e., <code>x</code> is close to 0), it computes    <code>log(-expm1(x))</code>. The <code>torch.expm1(x)</code> function computes <code>exp(x) - 1</code>    with high precision, avoiding cancellation.</li> <li>For <code>x &lt;= -ln(2)</code>, <code>exp(x)</code> is small, so the expression <code>1 - exp(x)</code>    is not problematic. For better precision, <code>log1p(-exp(x))</code> is used,    where <code>torch.log1p(y)</code> computes <code>log(1 + y)</code>.</li> </ol> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; log_p = torch.tensor([-1e-9, -2.0, -20.0])\n&gt;&gt;&gt; log1mexp(log_p)\ntensor([-2.0723e+01, -1.4541e-01, -2.0612e-09])\n</code></pre> Source code in <code>hierarchical_loss/utils.py</code> <pre><code>def log1mexp(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute log(1 - exp(x)) in a numerically stable way.\n\n    This function is designed to prevent the loss of precision that occurs\n    when `x` is very close to zero (i.e., a small negative number).\n    Directly computing `log(1 - exp(x))` can lead to catastrophic\n    cancellation and result in `-inf`.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor containing negative values (log-probabilities).\n        The function is not designed for `x &gt;= 0`, as `1 - exp(x)` would be\n        zero or negative, making the logarithm undefined.\n\n    Returns\n    -------\n    torch.Tensor\n        The computed `log(1 - exp(x))` values, with the same shape as `x`.\n\n    Notes\n    -----\n    The function uses two different mathematical identities based on the\n    value of `x` to ensure stability:\n\n    1. For `x &gt; -ln(2)` (i.e., `x` is close to 0), it computes\n       `log(-expm1(x))`. The `torch.expm1(x)` function computes `exp(x) - 1`\n       with high precision, avoiding cancellation.\n    2. For `x &lt;= -ln(2)`, `exp(x)` is small, so the expression `1 - exp(x)`\n       is not problematic. For better precision, `log1p(-exp(x))` is used,\n       where `torch.log1p(y)` computes `log(1 + y)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; log_p = torch.tensor([-1e-9, -2.0, -20.0])\n    &gt;&gt;&gt; log1mexp(log_p)\n    tensor([-2.0723e+01, -1.4541e-01, -2.0612e-09])\n\n\n    \"\"\"\n    # The threshold is -ln(2) approx -0.7\n    threshold = -0.7\n    # For x &gt; threshold, exp(x) is close to 1\n    result_close_to_zero = torch.log(-torch.expm1(x))\n    # For x &lt;= threshold, exp(x) is small\n    result_far_from_zero = torch.log1p(-torch.exp(x))\n\n    return torch.where(x &gt; threshold, result_close_to_zero, result_far_from_zero)\n</code></pre>"},{"location":"api/utils/#hierarchical_loss.utils.log_matrix","title":"<code>log_matrix(m)</code>","text":"<p>Formats a 2D array or tensor into a human-readable string.</p> <p>Useful for logging or debugging. Each row is prefixed with its index, and values are formatted to 4 decimal places.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>ndarray | Tensor</code> <p>The 2D array or tensor to format. Must support <code>.shape[0]</code>, iteration, and <code>.tolist()</code>.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A multi-line string representation of the matrix.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; t = torch.tensor([[1.0, 0.5], [0.25, 0.125]])\n&gt;&gt;&gt; print(log_matrix(t))\n0000: 1.0000, 0.5000\n0001: 0.2500, 0.1250\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; n = np.array([[0.1, 0.2], [0.3, 0.4]])\n&gt;&gt;&gt; print(log_matrix(n))\n0000: 0.1000, 0.2000\n0001: 0.3000, 0.4000\n</code></pre> Source code in <code>hierarchical_loss/utils.py</code> <pre><code>def log_matrix(m: np.ndarray | torch.Tensor) -&gt; str:\n    \"\"\"Formats a 2D array or tensor into a human-readable string.\n\n    Useful for logging or debugging. Each row is prefixed with its index,\n    and values are formatted to 4 decimal places.\n\n    Parameters\n    ----------\n    m : np.ndarray | torch.Tensor\n        The 2D array or tensor to format. Must support `.shape[0]`,\n        iteration, and `.tolist()`.\n\n    Returns\n    -------\n    str\n        A multi-line string representation of the matrix.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import torch\n    &gt;&gt;&gt; t = torch.tensor([[1.0, 0.5], [0.25, 0.125]])\n    &gt;&gt;&gt; print(log_matrix(t))\n    0000: 1.0000, 0.5000\n    0001: 0.2500, 0.1250\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; n = np.array([[0.1, 0.2], [0.3, 0.4]])\n    &gt;&gt;&gt; print(log_matrix(n))\n    0000: 0.1000, 0.2000\n    0001: 0.3000, 0.4000\n    \"\"\"\n    formatted_lines = []\n    for i in range(m.shape[0]):\n        vec = m[i]\n        line = f\"{i:04d}: \" + \", \".join(f\"{x:.4f}\" for x in vec.tolist())\n        formatted_lines.append(line)\n    return \"\\n\".join(formatted_lines)\n</code></pre>"},{"location":"api/utils/#hierarchical_loss.utils.logsumexp_over_siblings","title":"<code>logsumexp_over_siblings(flat_scores, sibling_mask)</code>","text":"<p>Computes logsumexp over sibling groups for each category.</p> <p>This function calculates the logsumexp of scores for all siblings within each group, and then populates the result for each category belonging to that group.</p> <p>Parameters:</p> Name Type Description Default <code>flat_scores</code> <code>Tensor</code> <p>raw scores for each category, batch-wise</p> required <code>sibling_mask</code> <code>Tensor</code> <p>a mask where sibling_mask[i,j] == sibling_mask[k,j] == 1 iff i and k are siblings.  Must be boolean.</p> required <p>Returns:</p> Name Type Description <code>logsumexp</code> <code>tensor(BxC)</code> <p>the logsumexp over all of the siblings of each category.  logsumexp[i,j] == logsumexp[i,k] if j,k are siblings.</p> <code>&gt;&gt;&gt; # Example 0: Normal operation</code> <code>&gt;&gt;&gt; flat_scores = torch.tensor([</code> <code>(...[0.1, 0.5, 2.0, 3.0],)</code> <code>...[0.8, 0.2, 1.5, 1.0]</code> <code>... ])</code> <code>&gt;&gt;&gt; sibling_mask = torch.tensor([</code> <code>(...[True, False],)</code> <code>(...[True, False],)</code> <code>(...[False, True],)</code> <code>...[False, True]</code> <code>... ], dtype=torch.bool)</code> <code>&gt;&gt;&gt; result = logsumexp_over_siblings(flat_scores, sibling_mask)</code> <code>&gt;&gt;&gt; torch.allclose(result, torch.tensor([</code> <code>(...[1.013, 1.013, 3.3133, 3.3133],)</code> <code>...     [1.2375, 1.2375, 1.9741, 1.9741]]), atol=1e-4)</code> <code>True</code> <code>&gt;&gt;&gt; # Example 1: Infinity Handling (Correctness Check)</code> <code>&gt;&gt;&gt; flat_scores_inf = torch.tensor([</code> <code>...[-inf, -inf, 2.0, inf]</code> <code>... ], dtype=torch.float32)</code> <code>&gt;&gt;&gt; result_inf = logsumexp_over_siblings(flat_scores_inf, sibling_mask)</code> <code>&gt;&gt;&gt; torch.allclose(result_inf, torch.tensor(</code> <code>...[[-inf, -inf, inf, inf]]</code> <code>... ), atol=1e-4)</code> <code>True</code> Source code in <code>hierarchical_loss/utils.py</code> <pre><code>def logsumexp_over_siblings(flat_scores: torch.Tensor, sibling_mask: torch.Tensor) -&gt; torch.Tensor:\n    '''Computes logsumexp over sibling groups for each category.\n\n    This function calculates the logsumexp of scores for all siblings\n    within each group, and then populates the result for each category\n    belonging to that group.\n\n    Parameters\n    ----------\n    flat_scores: tensor (BxC)\n        raw scores for each category, batch-wise\n    sibling_mask: tensor (CxG)\n        a mask where sibling_mask[i,j] == sibling_mask[k,j] == 1 iff i and k are siblings.  Must be boolean.\n\n    Returns\n    -------\n    logsumexp: tensor (BxC)\n        the logsumexp over all of the siblings of each category.  logsumexp[i,j] == logsumexp[i,k] if j,k are siblings.\n\n    &gt;&gt;&gt; # Example 0: Normal operation\n    &gt;&gt;&gt; flat_scores = torch.tensor([\n    ...     [0.1, 0.5, 2.0, 3.0],  # Batch 1\n    ...     [0.8, 0.2, 1.5, 1.0]   # Batch 2\n    ... ])\n    &gt;&gt;&gt; sibling_mask = torch.tensor([\n    ...     [True, False],\n    ...     [True, False],\n    ...     [False, True],\n    ...     [False, True]\n    ... ], dtype=torch.bool)\n    &gt;&gt;&gt; result = logsumexp_over_siblings(flat_scores, sibling_mask)\n    &gt;&gt;&gt; torch.allclose(result, torch.tensor([\n    ...     [1.0130, 1.0130, 3.3133, 3.3133],\n    ...     [1.2375, 1.2375, 1.9741, 1.9741]]), atol=1e-4)\n    True\n    &gt;&gt;&gt; # Example 1: Infinity Handling (Correctness Check)\n    &gt;&gt;&gt; flat_scores_inf = torch.tensor([\n    ...     [-torch.inf, -torch.inf, 2.0, torch.inf]\n    ... ], dtype=torch.float32)\n    &gt;&gt;&gt; result_inf = logsumexp_over_siblings(flat_scores_inf, sibling_mask)\n    &gt;&gt;&gt; torch.allclose(result_inf, torch.tensor(\n    ...     [[-torch.inf, -torch.inf, torch.inf, torch.inf]]\n    ... ), atol=1e-4)\n    True\n    '''\n\n    # B, C = flat_scores.shape\n    # G = sibling_mask.shape[1]\n    scores_expanded = flat_scores.unsqueeze(2)  # (B, C, 1)\n\n    mask_bool = sibling_mask.unsqueeze(0)  # (1, C, G)\n    masked_scores = torch.where(\n        mask_bool, scores_expanded, -torch.inf\n    )  # (B, C, G)\n\n    logsumexp_by_group = torch.logsumexp(masked_scores, dim=1)  # (B, G)\n\n    lse_expanded = logsumexp_by_group.unsqueeze(1)  # (B, 1, G)\n\n    #logsumexp_by_group is by group.  We want to replicate that over categories.\n    logsumexp_by_group_category_map = torch.where(\n        mask_bool,           # (1, C, G)\n        lse_expanded,        # (B, 1, G) -&gt; broadcasts to (B, C, G)\n        0.0                  # Scalar\n    ) # (B, C, G)\n\n    #logsumexp_by_group_category_map should have exactly one non-zero group entry per category, so we just sum over groups.\n    logsumexp = logsumexp_by_group_category_map.sum(dim=-1)  # (B, C)\n\n    return logsumexp\n</code></pre>"},{"location":"api/viz_utils/","title":"Visualization utilities","text":""},{"location":"api/viz_utils/#hierarchical_loss.viz_utils.draw_boxes_on_image","title":"<code>draw_boxes_on_image(pil_img, boxes, labels=None, scores=None, box_color=(0, 255, 0), text_color=(255, 255, 255), scaled=(640, 640))</code>","text":"<p>Draws bounding boxes with optional labels and scores onto an image.</p> <p>This function is vectorized and handles scaling of boxes internally.</p> <p>Parameters:</p> Name Type Description Default <code>pil_img</code> <code>Image</code> <p>The original image in PIL format.</p> required <code>boxes</code> <code>ndarray | Tensor</code> <p>An array or tensor of shape (R, 4) containing R boxes in [x1, y1, x2, y2] format.</p> required <code>labels</code> <code>list[str]</code> <p>A list of class names for each box.</p> <code>None</code> <code>scores</code> <code>list[float]</code> <p>A list of confidence scores for each box.</p> <code>None</code> <code>box_color</code> <code>tuple[int, int, int]</code> <p>The (B, G, R) color for the bounding boxes, by default (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>text_color</code> <code>tuple[int, int, int]</code> <p>The (B, G, R) color for the text labels, by default (255, 255, 255).</p> <code>(255, 255, 255)</code> <code>scaled</code> <code>tuple[int, int]</code> <p>The (height, width) shape the boxes are scaled from (i.e., the model input shape), by default (640, 640).</p> <code>(640, 640)</code> <p>Returns:</p> Type Description <code>Image</code> <p>A new PIL Image with the annotations drawn on it.</p> Source code in <code>hierarchical_loss/viz_utils.py</code> <pre><code>def draw_boxes_on_image(\n    pil_img: Image.Image,\n    boxes: np.ndarray | torch.Tensor,\n    labels: list[str] | None = None,\n    scores: list[float] | None = None,\n    box_color: tuple[int, int, int] = (0, 255, 0),\n    text_color: tuple[int, int, int] = (255, 255, 255),\n    scaled: tuple[int, int] = (640, 640),\n) -&gt; Image.Image:\n    \"\"\"Draws bounding boxes with optional labels and scores onto an image.\n\n    This function is vectorized and handles scaling of boxes internally.\n\n    Parameters\n    ----------\n    pil_img : Image.Image\n        The original image in PIL format.\n    boxes : np.ndarray | torch.Tensor\n        An array or tensor of shape (R, 4) containing R boxes in\n        [x1, y1, x2, y2] format.\n    labels : list[str], optional\n        A list of class names for each box.\n    scores : list[float], optional\n        A list of confidence scores for each box.\n    box_color : tuple[int, int, int], optional\n        The (B, G, R) color for the bounding boxes, by default (0, 255, 0).\n    text_color : tuple[int, int, int], optional\n        The (B, G, R) color for the text labels, by default (255, 255, 255).\n    scaled : tuple[int, int], optional\n        The (height, width) shape the boxes are scaled *from* (i.e., the\n        model input shape), by default (640, 640).\n\n    Returns\n    -------\n    Image.Image\n        A new PIL Image with the annotations drawn on it.\n    \"\"\"\n    img_cv = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n\n    img_area = pil_img.size[::-1][0] * pil_img.size[::-1][1]\n    expected_area = 1080*1920\n    area_scale_factor = 4 * img_area / expected_area \n\n    rescaled_boxes = rescale_boxes(boxes, scaled, pil_img.size[::-1])\n\n    for i, box in enumerate(rescaled_boxes):\n\n        x1, y1, x2, y2 = map(int, box)\n        cv2.rectangle(img_cv, (x1, y1), (x2, y2), box_color, 2)\n\n        label_text = ''\n        if labels is not None:\n            label_text += labels[i]\n        if scores is not None:\n            label_text += f' {scores[i]:.2f}' if label_text else f'{scores[i]:.2f}'\n\n        if label_text:\n            cv2.putText(img_cv, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, area_scale_factor, text_color, 2)\n\n    return Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n</code></pre>"},{"location":"api/viz_utils/#hierarchical_loss.viz_utils.rescale_boxes","title":"<code>rescale_boxes(pred_boxes, from_shape, to_shape)</code>","text":"<p>Rescales predicted boxes from model input shape to original image shape.</p> <p>This function works for both NumPy arrays and PyTorch tensors.</p> <p>Parameters:</p> Name Type Description Default <code>pred_boxes</code> <code>ndrray | Tensor</code> <p>An array or tensor of shape (..., 4) containing boxes in [x1, y1, x2, y2] format.</p> required <code>from_shape</code> <code>tuple[int, int]</code> <p>The original (height, width) of the model input, e.g., (640, 640).</p> required <code>to_shape</code> <code>Tuple[int, int]</code> <p>The target (height, width) of the original image.</p> required <p>Returns:</p> Type Description <code>ndarray | Tensor</code> <p>The rescaled boxes, in the same type as <code>pred_boxes</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; boxes_np = np.array([[10, 10, 60, 60]], dtype=np.float32)\n&gt;&gt;&gt; rescale_boxes(boxes_np, from_shape=(100, 100), to_shape=(200, 400))\narray([[ 40.,  20., 240., 120.]], dtype=float32)\n&gt;&gt;&gt; boxes_torch = torch.tensor([[10, 10, 60, 60]], dtype=torch.float32)\n&gt;&gt;&gt; rescale_boxes(boxes_torch, from_shape=(100, 100), to_shape=(200, 400))\ntensor([[ 40.,  20., 240., 120.]])\n</code></pre> Source code in <code>hierarchical_loss/viz_utils.py</code> <pre><code>def rescale_boxes(\n    pred_boxes: np.ndarray | torch.Tensor,\n    from_shape: tuple[int, int],\n    to_shape: tuple[int, int],\n) -&gt; np.ndarray | torch.Tensor:\n    \"\"\"Rescales predicted boxes from model input shape to original image shape.\n\n    This function works for both NumPy arrays and PyTorch tensors.\n\n    Parameters\n    ----------\n    pred_boxes : np.ndrray | torch.Tensor\n        An array or tensor of shape (..., 4) containing boxes in\n        [x1, y1, x2, y2] format.\n    from_shape : tuple[int, int]\n        The original (height, width) of the model input, e.g., (640, 640).\n    to_shape : Tuple[int, int]\n        The target (height, width) of the original image.\n\n    Returns\n    -------\n    np.ndarray | torch.Tensor\n        The rescaled boxes, in the same type as `pred_boxes`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; boxes_np = np.array([[10, 10, 60, 60]], dtype=np.float32)\n    &gt;&gt;&gt; rescale_boxes(boxes_np, from_shape=(100, 100), to_shape=(200, 400))\n    array([[ 40.,  20., 240., 120.]], dtype=float32)\n    &gt;&gt;&gt; boxes_torch = torch.tensor([[10, 10, 60, 60]], dtype=torch.float32)\n    &gt;&gt;&gt; rescale_boxes(boxes_torch, from_shape=(100, 100), to_shape=(200, 400))\n    tensor([[ 40.,  20., 240., 120.]])\n    \"\"\"\n    gain_w = to_shape[1] / from_shape[1]\n    gain_h = to_shape[0] / from_shape[0]\n\n    if hasattr(pred_boxes, \"new_tensor\"):\n        gain = pred_boxes.new_tensor([gain_w, gain_h, gain_w, gain_h])\n    else:\n        gain = np.array([gain_w, gain_h, gain_w, gain_h], dtype=pred_boxes.dtype)\n\n    return pred_boxes * gain\n</code></pre>"},{"location":"api/worms_utils/","title":"WORMS utilities","text":""},{"location":"api/worms_utils/#hierarchical_loss.worms_utils.WORMS_tree_to_childparent_tree","title":"<code>WORMS_tree_to_childparent_tree(worms_trees)</code>","text":"<p>Converts one or more WORMS classification trees into a {child: parent} dict.</p> <p>This function processes a list of nested tree structures (as returned by <code>get_WORMS_tree</code>) and flattens them into a single dictionary that maps each child AphiaID to its immediate parent AphiaID.</p> <p>Parameters:</p> Name Type Description Default <code>worms_trees</code> <code>list[dict]</code> <p>A list of nested tree structures from the WORMS API.</p> required <p>Returns:</p> Type Description <code>dict[int, int]</code> <p>A single dictionary representing the hierarchy in {child_AphiaID: parent_AphiaID} format.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tree1 = {\n...   \"AphiaID\": 1, \"scientificname\": \"Biota\", \"child\": {\n...     \"AphiaID\": 2, \"scientificname\": \"Animalia\", \"child\": {\n...       \"AphiaID\": 1821, \"scientificname\": \"Chordata\", \"child\": None\n...     }\n...   }\n... }\n&gt;&gt;&gt; tree2 = {\n...     \"AphiaID\": 1,\n...     \"rank\": \"Superdomain\",\n...     \"scientificname\": \"Biota\",\n...     \"child\": {\n...         \"AphiaID\": 2,\n...         \"rank\": \"Kingdom\",\n...         \"scientificname\": \"Animalia\",\n...         \"child\": {\n...             \"AphiaID\": 1065,\n...             \"rank\": \"Phylum\",\n...             \"scientificname\": \"Arthropoda\",\n...             \"child\": {\n...                 \"AphiaID\": 1274,\n...                 \"rank\": \"Subphylum\",\n...                 \"scientificname\": \"Chelicerata\",\n...                 \"child\": {\n...                     \"AphiaID\": 1300,\n...                     \"rank\": \"Class\",\n...                     \"scientificname\": \"Arachnida\",\n...                     \"child\": None\n...                 }\n...             }\n...         }\n...     }\n... }\n&gt;&gt;&gt; WORMS_tree_to_childparent_tree([tree1, tree2])\n{2: 1, 1821: 2, 1065: 2, 1274: 1065, 1300: 1274}\n</code></pre> Source code in <code>hierarchical_loss/worms_utils.py</code> <pre><code>def WORMS_tree_to_childparent_tree(worms_trees: list[dict]) -&gt; dict[int, int]:\n    \"\"\"Converts one or more WORMS classification trees into a {child: parent} dict.\n\n    This function processes a list of nested tree structures (as returned by\n    `get_WORMS_tree`) and flattens them into a single dictionary that maps\n    each child AphiaID to its immediate parent AphiaID.\n\n    Parameters\n    ----------\n    worms_trees : list[dict]\n        A list of nested tree structures from the WORMS API.\n\n    Returns\n    -------\n    dict[int, int]\n        A single dictionary representing the hierarchy in\n        {child_AphiaID: parent_AphiaID} format.\n\n    Examples\n    --------\n    &gt;&gt;&gt; tree1 = {\n    ...   \"AphiaID\": 1, \"scientificname\": \"Biota\", \"child\": {\n    ...     \"AphiaID\": 2, \"scientificname\": \"Animalia\", \"child\": {\n    ...       \"AphiaID\": 1821, \"scientificname\": \"Chordata\", \"child\": None\n    ...     }\n    ...   }\n    ... }\n    &gt;&gt;&gt; tree2 = {\n    ...     \"AphiaID\": 1,\n    ...     \"rank\": \"Superdomain\",\n    ...     \"scientificname\": \"Biota\",\n    ...     \"child\": {\n    ...         \"AphiaID\": 2,\n    ...         \"rank\": \"Kingdom\",\n    ...         \"scientificname\": \"Animalia\",\n    ...         \"child\": {\n    ...             \"AphiaID\": 1065,\n    ...             \"rank\": \"Phylum\",\n    ...             \"scientificname\": \"Arthropoda\",\n    ...             \"child\": {\n    ...                 \"AphiaID\": 1274,\n    ...                 \"rank\": \"Subphylum\",\n    ...                 \"scientificname\": \"Chelicerata\",\n    ...                 \"child\": {\n    ...                     \"AphiaID\": 1300,\n    ...                     \"rank\": \"Class\",\n    ...                     \"scientificname\": \"Arachnida\",\n    ...                     \"child\": None\n    ...                 }\n    ...             }\n    ...         }\n    ...     }\n    ... }\n    &gt;&gt;&gt; WORMS_tree_to_childparent_tree([tree1, tree2])\n    {2: 1, 1821: 2, 1065: 2, 1274: 1065, 1300: 1274}\n    \"\"\"\n    childparent_tree = {}\n    for tree in worms_trees:\n        try:\n            parent = tree['AphiaID']\n        except Exception as e:\n            print(\"could not find id\")\n            print(tree)\n            raise e\n        while 'child' in tree and tree['child']:\n            tree = tree['child']\n            try:\n                child = tree['AphiaID']\n            except Exception as e:\n                print(\"could not find id\")\n                print(tree)\n                raise e\n            childparent_tree[child] = parent\n            parent = child\n    return childparent_tree\n</code></pre>"},{"location":"api/worms_utils/#hierarchical_loss.worms_utils.get_WORMS_id","title":"<code>get_WORMS_id(name)</code>","text":"<p>Fetches the AphiaID from WORMS for a given scientific name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The scientific name of the organism to look up.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The corresponding AphiaID from the WORMS database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_WORMS_id('Gnathostomata')\n1828\n</code></pre> Source code in <code>hierarchical_loss/worms_utils.py</code> <pre><code>def get_WORMS_id(name: str) -&gt; int:\n    \"\"\"Fetches the AphiaID from WORMS for a given scientific name.\n\n    Parameters\n    ----------\n    name : str\n        The scientific name of the organism to look up.\n\n    Returns\n    -------\n    int\n        The corresponding AphiaID from the WORMS database.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_WORMS_id('Gnathostomata')\n    1828\n    \"\"\"\n    result = requests.get(WORMS_ID_URL.format(name))\n    return int(result.content)\n</code></pre>"},{"location":"api/worms_utils/#hierarchical_loss.worms_utils.get_WORMS_name","title":"<code>get_WORMS_name(WORMS_id)</code>","text":"<p>Fetches the scientific name from WORMS for a given AphiaID.</p> <p>The returned name is stripped of the surrounding double quotes that the API returns.</p> <p>Parameters:</p> Name Type Description Default <code>WORMS_id</code> <code>int</code> <p>The AphiaID of the organism to look up.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The corresponding scientific name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_WORMS_name(1828)\n'Gnathostomata'\n</code></pre> Source code in <code>hierarchical_loss/worms_utils.py</code> <pre><code>def get_WORMS_name(WORMS_id: int) -&gt; str:\n    \"\"\"Fetches the scientific name from WORMS for a given AphiaID.\n\n    The returned name is stripped of the surrounding double quotes\n    that the API returns.\n\n    Parameters\n    ----------\n    WORMS_id : int\n        The AphiaID of the organism to look up.\n\n    Returns\n    -------\n    str\n        The corresponding scientific name.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_WORMS_name(1828)\n    'Gnathostomata'\n    \"\"\"\n    result = requests.get(WORMS_NAME_URL.format(WORMS_id))\n    return result.content.decode(\"utf-8\")[1:-1]\n</code></pre>"},{"location":"api/worms_utils/#hierarchical_loss.worms_utils.get_WORMS_tree","title":"<code>get_WORMS_tree(organism_id)</code>","text":"<p>Fetches the full hierarchical classification tree from WORMS.</p> <p>Given an AphiaID or scientific name, retrieves the classification hierarchy from the root (\"Biota\") down to the specified organism.</p> <p>Parameters:</p> Name Type Description Default <code>organism_id</code> <code>int | str</code> <p>The AphiaID or scientific name of the organism.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A nested dictionary representing the classification tree.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import json\n&gt;&gt;&gt; print(json.dumps(get_WORMS_tree(get_WORMS_id('Gnathostomata')), indent=4))\n{\n    \"AphiaID\": 1,\n    \"rank\": \"Superdomain\",\n    \"scientificname\": \"Biota\",\n    \"child\": {\n        \"AphiaID\": 2,\n        \"rank\": \"Kingdom\",\n        \"scientificname\": \"Animalia\",\n        \"child\": {\n            \"AphiaID\": 1821,\n            \"rank\": \"Phylum\",\n            \"scientificname\": \"Chordata\",\n            \"child\": {\n                \"AphiaID\": 146419,\n                \"rank\": \"Subphylum\",\n                \"scientificname\": \"Vertebrata\",\n                \"child\": {\n                    \"AphiaID\": 1828,\n                    \"rank\": \"Infraphylum\",\n                    \"scientificname\": \"Gnathostomata\",\n                    \"child\": null\n                }\n            }\n        }\n    }\n}\n</code></pre> Source code in <code>hierarchical_loss/worms_utils.py</code> <pre><code>def get_WORMS_tree(organism_id: int | str) -&gt; dict:\n    \"\"\"Fetches the full hierarchical classification tree from WORMS.\n\n    Given an AphiaID or scientific name, retrieves the classification\n    hierarchy from the root (\"Biota\") down to the specified organism.\n\n    Parameters\n    ----------\n    organism_id : int | str\n        The AphiaID or scientific name of the organism.\n\n    Returns\n    -------\n    dict\n        A nested dictionary representing the classification tree.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import json\n    &gt;&gt;&gt; print(json.dumps(get_WORMS_tree(get_WORMS_id('Gnathostomata')), indent=4))\n    {\n        \"AphiaID\": 1,\n        \"rank\": \"Superdomain\",\n        \"scientificname\": \"Biota\",\n        \"child\": {\n            \"AphiaID\": 2,\n            \"rank\": \"Kingdom\",\n            \"scientificname\": \"Animalia\",\n            \"child\": {\n                \"AphiaID\": 1821,\n                \"rank\": \"Phylum\",\n                \"scientificname\": \"Chordata\",\n                \"child\": {\n                    \"AphiaID\": 146419,\n                    \"rank\": \"Subphylum\",\n                    \"scientificname\": \"Vertebrata\",\n                    \"child\": {\n                        \"AphiaID\": 1828,\n                        \"rank\": \"Infraphylum\",\n                        \"scientificname\": \"Gnathostomata\",\n                        \"child\": null\n                    }\n                }\n            }\n        }\n    }\n    \"\"\"\n    result = requests.get(WORMS_TREE_URL.format(organism_id))\n    return result.json()\n</code></pre>"}]}