import torch
from .hierarchy_tensor_utils import accumulate_hierarchy, expand_target_hierarchy
from .utils import log1mexp

def hierarchical_bce(
    pred: torch.Tensor, targets: torch.Tensor, hierarchy_index: torch.Tensor
) -> torch.Tensor:
    """Computes a hierarchical cross-entropy loss.

    This function interprets the raw `pred` logits as representing the
    log-odds of the *conditional* probability `P(category | parent)`.
    It then calculates the *marginal* probability `P(category)` by
    multiplying all conditional probabilities along the ancestor path.

    This multiplication is done in log-space for numerical stability.

    The loss is a variant of binary cross-entropy, calculated as:
    `-[t * log(s) + (1-t) * log(1-s)]`
    where:
    - `t` is the expanded target (1 for the node and all ancestors, 0 otherwise).
    - `s` is the *marginal* probability, `P(category)`.
    - `log(s)` is `sum(log(P(ancestor | parent_of_ancestor)))`, which is
      computed by `accumulate_hierarchy`.

    Parameters
    ----------
    pred : torch.Tensor
        The raw logit predictions from the model, with shape `(B, D, N)`,
        where `N` is the number of classes.
    targets : torch.Tensor
        The one-hot target tensor, with shape `(B, D, N)`.
    hierarchy_index : torch.Tensor
        An int tensor of shape `(N, M)` mapping each node `i` to its
        ancestral path `[i, parent, grandparent, ...]`.

    Returns
    -------
    torch.Tensor
        A tensor of the same shape as `pred` containing the loss
        value for each prediction.
    """
    # logsigmoids = log(P(category | parent))
    logsigmoids = torch.nn.functional.logsigmoid(pred)
    # This computes log(P(marginal)) = sum(log(P(c | p))) (Bayes' rule in log space)
    hierarchical_summed_logsigmoids = accumulate_hierarchy(logsigmoids, hierarchy_index, torch.sum, 0.)
    # Expand target to be 1 for the node and all its ancestors
    #targets = expand_target_hierarchy(targets, hierarchy_index)
    # log(1 - s) = log(1 - P(marginal))
    #            = log(1 - exp(log(P(marginal))))
    hierarchical_summed_log1sigmoids = log1mexp(hierarchical_summed_logsigmoids)
    # Standard BCE loss: -[t*log(s) + (1-t)*log(1-s)]
    return -(
      (targets * hierarchical_summed_logsigmoids) 
      + (1 - targets) * hierarchical_summed_log1sigmoids
    )

def hierarchical_conditional_bce(
    pred: torch.Tensor,
    targets: torch.Tensor,
    ancestor_mask: torch.Tensor,
    ancestor_sibling_mask: torch.Tensor
) -> torch.Tensor:
    """Computes a conditional Binary Cross Entropy loss for hierarchical data.

    This loss function enforces local consistency in a hierarchy. For a given
    target node, it treats:
    1. The node and all its ancestors as **Positives** (target 1).
    2. The "ancestor siblings" (uncles, great-uncles, siblings) as **Negatives** (target 0).
    3. All other nodes as **Ignored** (mask 0).

    This structure allows the model to learn `P(child | parent)` relationships
    without penalizing distant, unrelated branches of the tree.

    Parameters
    ----------
    pred : torch.Tensor
        The raw logits from the model, interpretable as `log(P(node | parent))`.
        Shape can be `(B, N)` or `(B, D, N)` where `N` is the number of classes.
    targets : torch.Tensor
        The one-hot target tensor matching the shape of `pred`.
    ancestor_mask : torch.Tensor
        A boolean tensor of shape `(N, N)` where `mask[i, j]` is True if `j`
        is an ancestor of `i`. Generated by `build_ancestor_mask`.
    ancestor_sibling_mask : torch.Tensor
        A boolean tensor of shape `(N, N)` where `mask[i, j]` is True if `j`
        is an ancestor sibling of `i`. Generated by `build_ancestor_sibling_mask`.

    Returns
    -------
    torch.Tensor
        A tensor matching the shape of `pred` containing the masked loss per element.
        The loss is 0.0 for ignored nodes.

    Examples
    --------
    >>> # Simple Tree: 0(root) -> 1, 2.  1 -> 3.
    >>> # Target is Node 3.
    >>> # Ancestors (Pos): {3, 1, 0}
    >>> # Ancestor Siblings (Neg): {2} (Node 2 is sibling of ancestor 1)
    >>>
    >>> # Mock Data
    >>> pred = torch.tensor([[ 10.0,  10.0, -10.0,  10.0]]) # Model predicts 0,1,3 high, 2 low
    >>> targets = torch.tensor([[0.0, 0.0, 0.0, 1.0]])      # Target is 3
    >>>
    >>> # Mock Masks (N=4)
    >>> anc_mask = torch.tensor([
    ...     [1, 0, 0, 0], [1, 1, 0, 0], [1, 0, 1, 0], [1, 1, 0, 1]
    ... ], dtype=torch.bool)
    >>> anc_sib_mask = torch.tensor([
    ...     [0, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 1, 0]
    ... ], dtype=torch.bool)
    >>>
    >>> loss = hierarchical_conditional_bce(pred, targets, anc_mask, anc_sib_mask)
    >>> # We expect low loss because prediction matches the hierarchy perfectly:
    >>> # Positives (0,1,3) are high, Negative (2) is low.
    >>> round(loss.sum().item(), 4)
    0.0002
    """
    # 1. Identify the leaf class index for each sample
    # Supports (B, N) or (B, D, N) by reducing the last dimension
    target_indices = targets.argmax(dim=-1)

    # 2. Retrieve the relevant masks for this batch
    # Broadcasting handles the extra dimensions in target_indices automatically.
    # If target_indices is (B, D), output is (B, D, N)
    positive_mask = ancestor_mask[target_indices]
    negative_mask = ancestor_sibling_mask[target_indices]

    # 3. Compute component-wise BCE
    # We use -logsigmoid(x) for positives (log(p))
    # We use -logsigmoid(-x) for negatives (log(1-p))
    loss_pos = -torch.nn.functional.logsigmoid(pred)
    loss_neg = -torch.nn.functional.logsigmoid(-pred)

    # 4. Apply masks and combine
    # Zeros out any node that is neither an ancestor nor an uncle
    return (positive_mask * loss_pos) + (negative_mask * loss_neg)
